digraph Tree {
node [shape=box, style="rounded", color="black", fontname=helvetica] ;
edge [fontname=helvetica] ;
0 [label="X[8] <= 0.12\nentropy = 3.91\nsamples = 100.0%\nvalue = [0.07, 0.06, 0.07, 0.07, 0.07, 0.07, 0.06, 0.07, 0.07\n0.07, 0.07, 0.06, 0.07, 0.07, 0.06]\nclass = New York Post"] ;
1 [label="X[1] <= -0.03\nentropy = 3.89\nsamples = 96.2%\nvalue = [0.07, 0.06, 0.07, 0.07, 0.07, 0.07, 0.03, 0.07, 0.08\n0.07, 0.07, 0.07, 0.07, 0.07, 0.07]\nclass = New York Post"] ;
0 -> 1 [labeldistance=2.5, labelangle=45, headlabel="True"] ;
2 [label="X[5] <= 0.04\nentropy = 3.83\nsamples = 57.6%\nvalue = [0.06, 0.05, 0.07, 0.06, 0.08, 0.09, 0.05, 0.09, 0.1\n0.05, 0.02, 0.08, 0.07, 0.05, 0.08]\nclass = New York Post"] ;
1 -> 2 ;
3 [label="X[3] <= -0.03\nentropy = 3.77\nsamples = 41.2%\nvalue = [0.08, 0.05, 0.08, 0.05, 0.09, 0.11, 0.04, 0.02, 0.13\n0.06, 0.02, 0.09, 0.08, 0.05, 0.07]\nclass = New York Post"] ;
2 -> 3 ;
4 [label="X[0] <= 0.11\nentropy = 3.62\nsamples = 19.7%\nvalue = [0.04, 0.05, 0.07, 0.08, 0.1, 0.13, 0.04, 0.02, 0.1\n0.09, 0.01, 0.11, 0.11, 0.06, 0.0]\nclass = NPR"] ;
3 -> 4 ;
5 [label="X[688] <= -0.01\nentropy = 2.31\nsamples = 1.8%\nvalue = [0.0, 0.0, 0.05, 0.45, 0.06, 0.03, 0.0, 0.08, 0.23\n0.02, 0.0, 0.0, 0.08, 0.0, 0.0]\nclass = Business Insider"] ;
4 -> 5 ;
6 [label="X[385] <= -0.02\nentropy = 2.03\nsamples = 0.7%\nvalue = [0.0, 0.0, 0.12, 0.04, 0.0, 0.0, 0.0, 0.21, 0.5, 0.04\n0.0, 0.0, 0.08, 0.0, 0.0]\nclass = New York Post"] ;
5 -> 6 ;
7 [label="X[574] <= 0.0\nentropy = 2.05\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.25, 0.08, 0.0, 0.0, 0.0, 0.42, 0.0, 0.08\n0.0, 0.0, 0.17, 0.0, 0.0]\nclass = Reuters"] ;
6 -> 7 ;
8 [label="X[883] <= 0.01\nentropy = 1.46\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17\n0.0, 0.0, 0.33, 0.0, 0.0]\nclass = CNN"] ;
7 -> 8 ;
9 [label="X[269] <= 0.01\nentropy = 0.92\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33\n0.0, 0.0, 0.67, 0.0, 0.0]\nclass = Atlantic"] ;
8 -> 9 ;
10 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
9 -> 10 ;
11 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
9 -> 11 ;
12 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
8 -> 12 ;
13 [label="X[59] <= 0.0\nentropy = 0.65\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.17, 0.0, 0.0, 0.0, 0.83, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
7 -> 13 ;
14 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
13 -> 14 ;
15 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
13 -> 15 ;
16 [label="entropy = 0.0\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
6 -> 16 ;
17 [label="X[971] <= -0.01\nentropy = 1.43\nsamples = 1.2%\nvalue = [0.0, 0.0, 0.0, 0.71, 0.11, 0.05, 0.0, 0.0, 0.05, 0.0\n0.0, 0.0, 0.08, 0.0, 0.0]\nclass = Business Insider"] ;
5 -> 17 ;
18 [label="X[585] <= -0.01\nentropy = 1.56\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.29, 0.0, 0.0, 0.29, 0.0\n0.0, 0.0, 0.43, 0.0, 0.0]\nclass = Atlantic"] ;
17 -> 18 ;
19 [label="X[743] <= -0.0\nentropy = 1.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.5, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
18 -> 19 ;
20 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
19 -> 20 ;
21 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
19 -> 21 ;
22 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
18 -> 22 ;
23 [label="X[857] <= 0.01\nentropy = 0.55\nsamples = 0.8%\nvalue = [0.0, 0.0, 0.0, 0.87, 0.13, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
17 -> 23 ;
24 [label="entropy = 0.0\nsamples = 0.7%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
23 -> 24 ;
25 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
23 -> 25 ;
26 [label="X[8] <= -0.01\nentropy = 3.59\nsamples = 17.8%\nvalue = [0.04, 0.05, 0.07, 0.04, 0.1, 0.14, 0.04, 0.01, 0.09\n0.1, 0.02, 0.12, 0.11, 0.06, 0.0]\nclass = NPR"] ;
4 -> 26 ;
27 [label="X[96] <= -0.0\nentropy = 3.47\nsamples = 8.3%\nvalue = [0.08, 0.04, 0.06, 0.06, 0.16, 0.12, 0.0, 0.0, 0.03\n0.12, 0.03, 0.07, 0.12, 0.1, 0.0]\nclass = Buzzfeed News"] ;
26 -> 27 ;
28 [label="X[674] <= -0.01\nentropy = 3.26\nsamples = 4.2%\nvalue = [0.11, 0.01, 0.08, 0.08, 0.27, 0.08, 0.01, 0.0, 0.06\n0.1, 0.02, 0.09, 0.05, 0.05, 0.0]\nclass = Buzzfeed News"] ;
27 -> 28 ;
29 [label="X[14] <= -0.0\nentropy = 2.4\nsamples = 1.9%\nvalue = [0.0, 0.0, 0.05, 0.02, 0.42, 0.16, 0.02, 0.0, 0.0\n0.17, 0.03, 0.12, 0.02, 0.0, 0.0]\nclass = Buzzfeed News"] ;
28 -> 29 ;
30 [label="X[968] <= -0.0\nentropy = 1.38\nsamples = 0.9%\nvalue = [0.0, 0.0, 0.06, 0.0, 0.65, 0.0, 0.0, 0.0, 0.0, 0.0\n0.06, 0.24, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
29 -> 30 ;
31 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
30 -> 31 ;
32 [label="X[375] <= 0.03\nentropy = 0.77\nsamples = 0.7%\nvalue = [0.0, 0.0, 0.08, 0.0, 0.85, 0.0, 0.0, 0.0, 0.0, 0.0\n0.08, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
30 -> 32 ;
33 [label="entropy = 0.0\nsamples = 0.6%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
32 -> 33 ;
34 [label="X[570] <= -0.02\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.5, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
32 -> 34 ;
35 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
34 -> 35 ;
36 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
34 -> 36 ;
37 [label="X[308] <= 0.0\nentropy = 2.14\nsamples = 1.0%\nvalue = [0.0, 0.0, 0.03, 0.03, 0.17, 0.33, 0.03, 0.0, 0.0\n0.37, 0.0, 0.0, 0.03, 0.0, 0.0]\nclass = Vox"] ;
29 -> 37 ;
38 [label="X[70] <= -0.02\nentropy = 0.92\nsamples = 0.5%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.33, 0.67, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
37 -> 38 ;
39 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
38 -> 39 ;
40 [label="entropy = 0.0\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
38 -> 40 ;
41 [label="X[160] <= 0.01\nentropy = 1.37\nsamples = 0.5%\nvalue = [0.0, 0.0, 0.07, 0.07, 0.0, 0.0, 0.07, 0.0, 0.0, 0.73\n0.0, 0.0, 0.07, 0.0, 0.0]\nclass = Vox"] ;
37 -> 41 ;
42 [label="X[275] <= -0.0\nentropy = 2.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.25, 0.25, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0\n0.0, 0.0, 0.25, 0.0, 0.0]\nclass = CNN"] ;
41 -> 42 ;
43 [label="X[581] <= -0.01\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.5, 0.0, 0.0]\nclass = CNN"] ;
42 -> 43 ;
44 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
43 -> 44 ;
45 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
43 -> 45 ;
46 [label="X[726] <= 0.01\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
42 -> 46 ;
47 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
46 -> 47 ;
48 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
46 -> 48 ;
49 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
41 -> 49 ;
50 [label="X[181] <= 0.0\nentropy = 3.15\nsamples = 2.3%\nvalue = [0.21, 0.01, 0.12, 0.15, 0.12, 0.01, 0.0, 0.0, 0.12\n0.03, 0.0, 0.06, 0.07, 0.1, 0.0]\nclass = Breitbart"] ;
28 -> 50 ;
51 [label="X[431] <= -0.01\nentropy = 2.5\nsamples = 1.2%\nvalue = [0.37, 0.0, 0.05, 0.0, 0.05, 0.03, 0.0, 0.0, 0.21\n0.03, 0.0, 0.0, 0.13, 0.13, 0.0]\nclass = Breitbart"] ;
50 -> 51 ;
52 [label="X[670] <= -0.03\nentropy = 2.05\nsamples = 0.6%\nvalue = [0.0, 0.0, 0.1, 0.0, 0.05, 0.0, 0.0, 0.0, 0.38, 0.0\n0.0, 0.0, 0.24, 0.24, 0.0]\nclass = New York Post"] ;
51 -> 52 ;
53 [label="X[539] <= -0.0\nentropy = 0.72\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
52 -> 53 ;
54 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
53 -> 54 ;
55 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
53 -> 55 ;
56 [label="X[41] <= -0.01\nentropy = 1.35\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.45, 0.45, 0.0]\nclass = Atlantic"] ;
52 -> 56 ;
57 [label="X[301] <= -0.01\nentropy = 0.65\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.17, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.83, 0.0, 0.0]\nclass = Atlantic"] ;
56 -> 57 ;
58 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
57 -> 58 ;
59 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
57 -> 59 ;
60 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
56 -> 60 ;
61 [label="X[494] <= 0.01\nentropy = 0.95\nsamples = 0.6%\nvalue = [0.82, 0.0, 0.0, 0.0, 0.06, 0.06, 0.0, 0.0, 0.0, 0.06\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
51 -> 61 ;
62 [label="entropy = 0.0\nsamples = 0.4%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
61 -> 62 ;
63 [label="X[626] <= -0.03\nentropy = 1.58\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.33, 0.33, 0.0, 0.0, 0.0, 0.33\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
61 -> 63 ;
64 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
63 -> 64 ;
65 [label="X[801] <= -0.02\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.5\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
63 -> 65 ;
66 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
65 -> 66 ;
67 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
65 -> 67 ;
68 [label="X[245] <= -0.0\nentropy = 2.43\nsamples = 1.1%\nvalue = [0.0, 0.03, 0.2, 0.33, 0.2, 0.0, 0.0, 0.0, 0.0, 0.03\n0.0, 0.13, 0.0, 0.07, 0.0]\nclass = Business Insider"] ;
50 -> 68 ;
69 [label="X[428] <= -0.01\nentropy = 1.16\nsamples = 0.5%\nvalue = [0.0, 0.0, 0.0, 0.67, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07\n0.0, 0.27, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
68 -> 69 ;
70 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
69 -> 70 ;
71 [label="X[403] <= 0.02\nentropy = 0.72\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2\n0.0, 0.8, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
69 -> 71 ;
72 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
71 -> 72 ;
73 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
71 -> 73 ;
74 [label="X[198] <= -0.0\nentropy = 1.71\nsamples = 0.6%\nvalue = [0.0, 0.07, 0.4, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.13, 0.0]\nclass = CNN"] ;
68 -> 74 ;
75 [label="X[142] <= 0.01\nentropy = 0.81\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.25, 0.0]\nclass = CNN"] ;
74 -> 75 ;
76 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
75 -> 76 ;
77 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
75 -> 77 ;
78 [label="X[31] <= -0.04\nentropy = 0.59\nsamples = 0.2%\nvalue = [0.0, 0.14, 0.0, 0.0, 0.86, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
74 -> 78 ;
79 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
78 -> 79 ;
80 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
78 -> 80 ;
81 [label="X[19] <= -0.04\nentropy = 3.22\nsamples = 4.0%\nvalue = [0.05, 0.08, 0.03, 0.04, 0.04, 0.16, 0.0, 0.0, 0.0\n0.15, 0.05, 0.06, 0.2, 0.14, 0.01]\nclass = Atlantic"] ;
27 -> 81 ;
82 [label="X[335] <= 0.02\nentropy = 2.55\nsamples = 1.2%\nvalue = [0.0, 0.19, 0.0, 0.14, 0.14, 0.24, 0.0, 0.0, 0.0\n0.14, 0.16, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
81 -> 82 ;
83 [label="X[818] <= -0.01\nentropy = 1.93\nsamples = 0.8%\nvalue = [0.0, 0.28, 0.0, 0.0, 0.2, 0.36, 0.0, 0.0, 0.0, 0.16\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
82 -> 83 ;
84 [label="X[955] <= 0.0\nentropy = 0.95\nsamples = 0.4%\nvalue = [0.0, 0.64, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.36\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
83 -> 84 ;
85 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
84 -> 85 ;
86 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
84 -> 86 ;
87 [label="X[526] <= -0.01\nentropy = 0.94\nsamples = 0.5%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.36, 0.64, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
83 -> 87 ;
88 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
87 -> 88 ;
89 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
87 -> 89 ;
90 [label="X[355] <= 0.02\nentropy = 1.33\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.42, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08\n0.5, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
82 -> 90 ;
91 [label="X[24] <= 0.05\nentropy = 0.65\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.83, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
90 -> 91 ;
92 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
91 -> 92 ;
93 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
91 -> 93 ;
94 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
90 -> 94 ;
95 [label="X[799] <= -0.0\nentropy = 2.76\nsamples = 2.8%\nvalue = [0.07, 0.03, 0.04, 0.0, 0.0, 0.12, 0.0, 0.0, 0.0\n0.16, 0.0, 0.08, 0.28, 0.2, 0.01]\nclass = Atlantic"] ;
81 -> 95 ;
96 [label="X[395] <= 0.02\nentropy = 2.27\nsamples = 1.4%\nvalue = [0.0, 0.04, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.29\n0.0, 0.11, 0.11, 0.38, 0.02]\nclass = National Review"] ;
95 -> 96 ;
97 [label="X[835] <= 0.01\nentropy = 2.14\nsamples = 0.8%\nvalue = [0.0, 0.07, 0.0, 0.0, 0.0, 0.07, 0.0, 0.0, 0.0, 0.48\n0.0, 0.19, 0.11, 0.07, 0.0]\nclass = Vox"] ;
96 -> 97 ;
98 [label="X[794] <= -0.0\nentropy = 1.02\nsamples = 0.5%\nvalue = [0.0, 0.12, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.76\n0.0, 0.0, 0.0, 0.12, 0.0]\nclass = Vox"] ;
97 -> 98 ;
99 [label="X[305] <= 0.0\nentropy = 1.0\nsamples = 0.2%\nvalue = [0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.5, 0.0]\nclass = Washington Post"] ;
98 -> 99 ;
100 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
99 -> 100 ;
101 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
99 -> 101 ;
102 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
98 -> 102 ;
103 [label="X[472] <= -0.0\nentropy = 1.49\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0\n0.0, 0.5, 0.3, 0.0, 0.0]\nclass = Guardian"] ;
97 -> 103 ;
104 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
103 -> 104 ;
105 [label="X[454] <= -0.02\nentropy = 0.97\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.6, 0.0, 0.0]\nclass = Atlantic"] ;
103 -> 105 ;
106 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
105 -> 106 ;
107 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
105 -> 107 ;
108 [label="X[91] <= 0.03\nentropy = 0.8\nsamples = 0.6%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.11, 0.83, 0.06]\nclass = National Review"] ;
96 -> 108 ;
109 [label="entropy = 0.0\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
108 -> 109 ;
110 [label="X[575] <= 0.0\nentropy = 0.92\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.67, 0.0, 0.33]\nclass = Atlantic"] ;
108 -> 110 ;
111 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
110 -> 111 ;
112 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
110 -> 112 ;
113 [label="X[283] <= -0.01\nentropy = 2.27\nsamples = 1.4%\nvalue = [0.14, 0.02, 0.09, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.02\n0.0, 0.05, 0.45, 0.02, 0.0]\nclass = Atlantic"] ;
95 -> 113 ;
114 [label="X[838] <= 0.02\nentropy = 1.12\nsamples = 0.7%\nvalue = [0.04, 0.04, 0.17, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.75, 0.0, 0.0]\nclass = Atlantic"] ;
113 -> 114 ;
115 [label="entropy = 0.0\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
114 -> 115 ;
116 [label="X[718] <= 0.01\nentropy = 1.25\nsamples = 0.3%\nvalue = [0.17, 0.17, 0.67, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
114 -> 116 ;
117 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
116 -> 117 ;
118 [label="X[85] <= -0.03\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
116 -> 118 ;
119 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
118 -> 119 ;
120 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
118 -> 120 ;
121 [label="X[515] <= -0.01\nentropy = 2.11\nsamples = 0.7%\nvalue = [0.25, 0.0, 0.0, 0.0, 0.0, 0.45, 0.0, 0.0, 0.0, 0.05\n0.0, 0.1, 0.1, 0.05, 0.0]\nclass = NPR"] ;
113 -> 121 ;
122 [label="X[407] <= 0.01\nentropy = 1.3\nsamples = 0.3%\nvalue = [0.62, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.25, 0.0, 0.12, 0.0]\nclass = Breitbart"] ;
121 -> 122 ;
123 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
122 -> 123 ;
124 [label="X[747] <= -0.01\nentropy = 0.92\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.67, 0.0, 0.33, 0.0]\nclass = Guardian"] ;
122 -> 124 ;
125 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
124 -> 125 ;
126 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
124 -> 126 ;
127 [label="X[230] <= -0.01\nentropy = 1.04\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.08\n0.0, 0.0, 0.17, 0.0, 0.0]\nclass = NPR"] ;
121 -> 127 ;
128 [label="X[576] <= 0.03\nentropy = 0.92\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33\n0.0, 0.0, 0.67, 0.0, 0.0]\nclass = Atlantic"] ;
127 -> 128 ;
129 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
128 -> 129 ;
130 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
128 -> 130 ;
131 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
127 -> 131 ;
132 [label="X[29] <= 0.02\nentropy = 3.36\nsamples = 9.6%\nvalue = [0.01, 0.06, 0.08, 0.01, 0.04, 0.17, 0.07, 0.02, 0.14\n0.07, 0.0, 0.17, 0.11, 0.03, 0.0]\nclass = NPR"] ;
26 -> 132 ;
133 [label="X[169] <= 0.01\nentropy = 3.35\nsamples = 6.7%\nvalue = [0.01, 0.08, 0.11, 0.01, 0.06, 0.05, 0.07, 0.01, 0.15\n0.1, 0.0, 0.18, 0.12, 0.04, 0.0]\nclass = Guardian"] ;
132 -> 133 ;
134 [label="X[18] <= 0.0\nentropy = 3.2\nsamples = 4.4%\nvalue = [0.01, 0.1, 0.01, 0.02, 0.1, 0.08, 0.05, 0.0, 0.22\n0.1, 0.0, 0.17, 0.08, 0.07, 0.0]\nclass = New York Post"] ;
133 -> 134 ;
135 [label="X[888] <= 0.01\nentropy = 2.73\nsamples = 2.0%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.19, 0.08, 0.12, 0.0, 0.12\n0.02, 0.0, 0.27, 0.15, 0.06, 0.0]\nclass = Guardian"] ;
134 -> 135 ;
136 [label="X[686] <= 0.02\nentropy = 2.23\nsamples = 0.9%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.36, 0.0, 0.04, 0.0, 0.24, 0.04\n0.0, 0.0, 0.2, 0.12, 0.0]\nclass = Buzzfeed News"] ;
135 -> 136 ;
137 [label="X[92] <= 0.01\nentropy = 1.61\nsamples = 0.6%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.46, 0.0\n0.0, 0.0, 0.38, 0.08, 0.0]\nclass = New York Post"] ;
136 -> 137 ;
138 [label="X[572] <= -0.03\nentropy = 0.59\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.86, 0.0\n0.0, 0.0, 0.0, 0.14, 0.0]\nclass = New York Post"] ;
137 -> 138 ;
139 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
138 -> 139 ;
140 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
138 -> 140 ;
141 [label="X[499] <= 0.04\nentropy = 0.65\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17, 0.0, 0.0, 0.0\n0.0, 0.0, 0.83, 0.0, 0.0]\nclass = Atlantic"] ;
137 -> 141 ;
142 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
141 -> 142 ;
143 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
141 -> 143 ;
144 [label="X[380] <= 0.01\nentropy = 1.04\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.08\n0.0, 0.0, 0.0, 0.17, 0.0]\nclass = Buzzfeed News"] ;
136 -> 144 ;
145 [label="X[487] <= -0.0\nentropy = 0.92\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33\n0.0, 0.0, 0.0, 0.67, 0.0]\nclass = National Review"] ;
144 -> 145 ;
146 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
145 -> 146 ;
147 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
145 -> 147 ;
148 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
144 -> 148 ;
149 [label="X[736] <= 0.01\nentropy = 1.88\nsamples = 1.0%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.04, 0.15, 0.19, 0.0, 0.0, 0.0\n0.0, 0.52, 0.11, 0.0, 0.0]\nclass = Guardian"] ;
135 -> 149 ;
150 [label="X[822] <= 0.0\nentropy = 2.07\nsamples = 0.6%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.07, 0.29, 0.36, 0.0, 0.0, 0.0\n0.0, 0.07, 0.21, 0.0, 0.0]\nclass = New York Times"] ;
149 -> 150 ;
151 [label="X[13] <= -0.01\nentropy = 1.15\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.14, 0.0, 0.71, 0.0, 0.0, 0.0\n0.0, 0.14, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
150 -> 151 ;
152 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
151 -> 152 ;
153 [label="X[750] <= 0.02\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.5, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
151 -> 153 ;
154 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
153 -> 154 ;
155 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
153 -> 155 ;
156 [label="X[662] <= 0.01\nentropy = 0.99\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.57, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.43, 0.0, 0.0]\nclass = NPR"] ;
150 -> 156 ;
157 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
156 -> 157 ;
158 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
156 -> 158 ;
159 [label="entropy = 0.0\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
149 -> 159 ;
160 [label="X[319] <= 0.0\nentropy = 2.82\nsamples = 2.5%\nvalue = [0.02, 0.19, 0.02, 0.03, 0.02, 0.08, 0.0, 0.0, 0.3\n0.17, 0.0, 0.08, 0.02, 0.08, 0.0]\nclass = New York Post"] ;
134 -> 160 ;
161 [label="X[278] <= -0.01\nentropy = 2.11\nsamples = 0.9%\nvalue = [0.0, 0.04, 0.0, 0.08, 0.0, 0.2, 0.0, 0.0, 0.2, 0.44\n0.0, 0.04, 0.0, 0.0, 0.0]\nclass = Vox"] ;
160 -> 161 ;
162 [label="X[655] <= -0.02\nentropy = 0.41\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.92\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
161 -> 162 ;
163 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
162 -> 163 ;
164 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
162 -> 164 ;
165 [label="X[279] <= -0.0\nentropy = 2.04\nsamples = 0.6%\nvalue = [0.0, 0.08, 0.0, 0.15, 0.0, 0.38, 0.0, 0.0, 0.31, 0.0\n0.0, 0.08, 0.0, 0.0, 0.0]\nclass = NPR"] ;
161 -> 165 ;
166 [label="X[522] <= 0.0\nentropy = 1.25\nsamples = 0.3%\nvalue = [0.0, 0.17, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.67, 0.0\n0.0, 0.17, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
165 -> 166 ;
167 [label="X[310] <= -0.0\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.5, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
166 -> 167 ;
168 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
167 -> 168 ;
169 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
167 -> 169 ;
170 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
166 -> 170 ;
171 [label="X[729] <= -0.03\nentropy = 0.86\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.29, 0.0, 0.71, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
165 -> 171 ;
172 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
171 -> 172 ;
173 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
171 -> 173 ;
174 [label="X[718] <= -0.01\nentropy = 2.33\nsamples = 1.5%\nvalue = [0.03, 0.29, 0.03, 0.0, 0.03, 0.0, 0.0, 0.0, 0.37\n0.0, 0.0, 0.11, 0.03, 0.13, 0.0]\nclass = New York Post"] ;
160 -> 174 ;
175 [label="X[965] <= -0.01\nentropy = 1.12\nsamples = 0.6%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.69, 0.0\n0.0, 0.25, 0.06, 0.0, 0.0]\nclass = New York Post"] ;
174 -> 175 ;
176 [label="X[59] <= 0.05\nentropy = 0.72\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.8, 0.2, 0.0, 0.0]\nclass = Guardian"] ;
175 -> 176 ;
177 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
176 -> 177 ;
178 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
176 -> 178 ;
179 [label="entropy = 0.0\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
175 -> 179 ;
180 [label="X[930] <= 0.01\nentropy = 1.99\nsamples = 0.9%\nvalue = [0.05, 0.5, 0.05, 0.0, 0.05, 0.0, 0.0, 0.0, 0.14, 0.0\n0.0, 0.0, 0.0, 0.23, 0.0]\nclass = Washington Post"] ;
174 -> 180 ;
181 [label="X[322] <= 0.01\nentropy = 0.75\nsamples = 0.5%\nvalue = [0.0, 0.79, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
180 -> 181 ;
182 [label="entropy = 0.0\nsamples = 0.4%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
181 -> 182 ;
183 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
181 -> 183 ;
184 [label="X[573] <= 0.01\nentropy = 1.55\nsamples = 0.4%\nvalue = [0.12, 0.0, 0.12, 0.0, 0.12, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.62, 0.0]\nclass = National Review"] ;
180 -> 184 ;
185 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
184 -> 185 ;
186 [label="X[609] <= -0.01\nentropy = 1.58\nsamples = 0.2%\nvalue = [0.33, 0.0, 0.33, 0.0, 0.33, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
184 -> 186 ;
187 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
186 -> 187 ;
188 [label="X[265] <= -0.01\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.5, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
186 -> 188 ;
189 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
188 -> 189 ;
190 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
188 -> 190 ;
191 [label="X[33] <= 0.03\nentropy = 2.77\nsamples = 2.3%\nvalue = [0.01, 0.03, 0.28, 0.0, 0.0, 0.01, 0.09, 0.03, 0.04\n0.1, 0.0, 0.19, 0.19, 0.0, 0.0]\nclass = CNN"] ;
133 -> 191 ;
192 [label="X[890] <= 0.01\nentropy = 2.45\nsamples = 1.6%\nvalue = [0.02, 0.02, 0.39, 0.0, 0.0, 0.0, 0.12, 0.04, 0.06\n0.04, 0.0, 0.27, 0.04, 0.0, 0.0]\nclass = CNN"] ;
191 -> 192 ;
193 [label="X[270] <= -0.0\nentropy = 1.91\nsamples = 0.8%\nvalue = [0.0, 0.04, 0.04, 0.0, 0.0, 0.0, 0.17, 0.0, 0.0, 0.09\n0.0, 0.57, 0.09, 0.0, 0.0]\nclass = Guardian"] ;
192 -> 193 ;
194 [label="X[243] <= -0.02\nentropy = 1.38\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.14, 0.0, 0.0, 0.0, 0.57, 0.0, 0.0, 0.0\n0.0, 0.0, 0.29, 0.0, 0.0]\nclass = New York Times"] ;
193 -> 194 ;
195 [label="X[37] <= 0.03\nentropy = 0.92\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.33, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.67, 0.0, 0.0]\nclass = Atlantic"] ;
194 -> 195 ;
196 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
195 -> 196 ;
197 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
195 -> 197 ;
198 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
194 -> 198 ;
199 [label="X[860] <= -0.01\nentropy = 0.87\nsamples = 0.5%\nvalue = [0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12\n0.0, 0.81, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
193 -> 199 ;
200 [label="entropy = 0.0\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
199 -> 200 ;
201 [label="X[832] <= -0.0\nentropy = 0.92\nsamples = 0.2%\nvalue = [0.0, 0.33, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.67\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
199 -> 201 ;
202 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
201 -> 202 ;
203 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
201 -> 203 ;
204 [label="X[35] <= 0.01\nentropy = 1.48\nsamples = 0.8%\nvalue = [0.04, 0.0, 0.69, 0.0, 0.0, 0.0, 0.08, 0.08, 0.12\n0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
192 -> 204 ;
205 [label="X[160] <= -0.01\nentropy = 1.91\nsamples = 0.3%\nvalue = [0.12, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.25, 0.38, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
204 -> 205 ;
206 [label="X[571] <= 0.02\nentropy = 0.81\nsamples = 0.2%\nvalue = [0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
205 -> 206 ;
207 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
206 -> 207 ;
208 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
206 -> 208 ;
209 [label="X[470] <= -0.04\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
205 -> 209 ;
210 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
209 -> 210 ;
211 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
209 -> 211 ;
212 [label="entropy = 0.0\nsamples = 0.5%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
204 -> 212 ;
213 [label="X[56] <= -0.01\nentropy = 1.41\nsamples = 0.7%\nvalue = [0.0, 0.06, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.28\n0.0, 0.0, 0.61, 0.0, 0.0]\nclass = Atlantic"] ;
191 -> 213 ;
214 [label="X[742] <= 0.02\nentropy = 0.41\nsamples = 0.4%\nvalue = [0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.92, 0.0, 0.0]\nclass = Atlantic"] ;
213 -> 214 ;
215 [label="entropy = 0.0\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
214 -> 215 ;
216 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
214 -> 216 ;
217 [label="X[900] <= -0.02\nentropy = 0.65\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.17, 0.0, 0.0, 0.0, 0.83\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
213 -> 217 ;
218 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
217 -> 218 ;
219 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
217 -> 219 ;
220 [label="X[74] <= -0.0\nentropy = 2.72\nsamples = 2.8%\nvalue = [0.0, 0.02, 0.02, 0.02, 0.01, 0.41, 0.09, 0.05, 0.12\n0.0, 0.0, 0.15, 0.08, 0.01, 0.01]\nclass = NPR"] ;
132 -> 220 ;
221 [label="X[193] <= 0.03\nentropy = 2.86\nsamples = 1.3%\nvalue = [0.0, 0.06, 0.03, 0.03, 0.0, 0.06, 0.06, 0.06, 0.24\n0.0, 0.0, 0.24, 0.21, 0.03, 0.0]\nclass = New York Post"] ;
220 -> 221 ;
222 [label="X[558] <= -0.0\nentropy = 2.52\nsamples = 0.9%\nvalue = [0.0, 0.0, 0.04, 0.04, 0.0, 0.08, 0.08, 0.08, 0.0\n0.0, 0.0, 0.33, 0.29, 0.04, 0.0]\nclass = Guardian"] ;
221 -> 222 ;
223 [label="X[480] <= 0.0\nentropy = 1.42\nsamples = 0.5%\nvalue = [0.0, 0.0, 0.08, 0.08, 0.0, 0.17, 0.0, 0.0, 0.0, 0.0\n0.0, 0.67, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
222 -> 223 ;
224 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
223 -> 224 ;
225 [label="X[907] <= 0.02\nentropy = 1.5\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.25, 0.25, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
223 -> 225 ;
226 [label="X[609] <= 0.01\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
225 -> 226 ;
227 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
226 -> 227 ;
228 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
226 -> 228 ;
229 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
225 -> 229 ;
230 [label="X[452] <= -0.0\nentropy = 1.61\nsamples = 0.5%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17, 0.17, 0.0, 0.0\n0.0, 0.0, 0.58, 0.08, 0.0]\nclass = Atlantic"] ;
222 -> 230 ;
231 [label="X[476] <= -0.0\nentropy = 1.52\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.4, 0.0, 0.0\n0.0, 0.0, 0.0, 0.2, 0.0]\nclass = New York Times"] ;
230 -> 231 ;
232 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
231 -> 232 ;
233 [label="X[407] <= -0.01\nentropy = 0.92\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.67, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.33, 0.0]\nclass = New York Times"] ;
231 -> 233 ;
234 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
233 -> 234 ;
235 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
233 -> 235 ;
236 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
230 -> 236 ;
237 [label="X[129] <= -0.02\nentropy = 0.72\nsamples = 0.3%\nvalue = [0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
221 -> 237 ;
238 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
237 -> 238 ;
239 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
237 -> 239 ;
240 [label="X[375] <= -0.02\nentropy = 1.9\nsamples = 1.6%\nvalue = [0.0, 0.0, 0.02, 0.02, 0.02, 0.63, 0.12, 0.04, 0.04\n0.0, 0.0, 0.1, 0.0, 0.0, 0.02]\nclass = NPR"] ;
220 -> 240 ;
241 [label="X[701] <= 0.0\nentropy = 1.73\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.5, 0.17, 0.0, 0.0\n0.0, 0.25, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
240 -> 241 ;
242 [label="X[754] <= 0.01\nentropy = 1.46\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.17, 0.0, 0.0, 0.0, 0.0, 0.33, 0.0, 0.0\n0.0, 0.5, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
241 -> 242 ;
243 [label="X[549] <= -0.02\nentropy = 0.92\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.33, 0.0, 0.0, 0.0, 0.0, 0.67, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
242 -> 243 ;
244 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
243 -> 244 ;
245 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
243 -> 245 ;
246 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
242 -> 246 ;
247 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
241 -> 247 ;
248 [label="X[664] <= 0.02\nentropy = 1.06\nsamples = 1.2%\nvalue = [0.0, 0.0, 0.0, 0.02, 0.02, 0.82, 0.0, 0.0, 0.05, 0.0\n0.0, 0.05, 0.0, 0.0, 0.02]\nclass = NPR"] ;
240 -> 248 ;
249 [label="X[648] <= -0.03\nentropy = 0.37\nsamples = 1.0%\nvalue = [0.0, 0.0, 0.0, 0.03, 0.0, 0.94, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.03]\nclass = NPR"] ;
248 -> 249 ;
250 [label="X[234] <= -0.03\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.5]\nclass = Business Insider"] ;
249 -> 250 ;
251 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
250 -> 251 ;
252 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
250 -> 252 ;
253 [label="entropy = 0.0\nsamples = 0.9%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
249 -> 253 ;
254 [label="X[678] <= 0.04\nentropy = 1.52\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.4, 0.0\n0.0, 0.4, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
248 -> 254 ;
255 [label="X[15] <= -0.06\nentropy = 0.92\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.33, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.67, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
254 -> 255 ;
256 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
255 -> 256 ;
257 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
255 -> 257 ;
258 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
254 -> 258 ;
259 [label="X[0] <= 0.2\nentropy = 3.67\nsamples = 21.5%\nvalue = [0.11, 0.05, 0.1, 0.02, 0.08, 0.08, 0.04, 0.02, 0.15\n0.03, 0.03, 0.07, 0.05, 0.05, 0.13]\nclass = New York Post"] ;
3 -> 259 ;
260 [label="X[14] <= -0.02\nentropy = 3.38\nsamples = 13.3%\nvalue = [0.16, 0.05, 0.05, 0.02, 0.06, 0.07, 0.03, 0.02, 0.23\n0.01, 0.03, 0.08, 0.03, 0.02, 0.14]\nclass = New York Post"] ;
259 -> 260 ;
261 [label="X[36] <= -0.01\nentropy = 2.8\nsamples = 6.1%\nvalue = [0.26, 0.02, 0.01, 0.02, 0.09, 0.02, 0.0, 0.0, 0.3\n0.01, 0.05, 0.07, 0.01, 0.02, 0.11]\nclass = New York Post"] ;
260 -> 261 ;
262 [label="X[212] <= 0.02\nentropy = 2.45\nsamples = 2.5%\nvalue = [0.42, 0.0, 0.01, 0.04, 0.08, 0.0, 0.0, 0.0, 0.17\n0.01, 0.13, 0.11, 0.0, 0.01, 0.0]\nclass = Breitbart"] ;
261 -> 262 ;
263 [label="X[773] <= -0.02\nentropy = 1.55\nsamples = 1.6%\nvalue = [0.62, 0.0, 0.02, 0.07, 0.07, 0.0, 0.0, 0.0, 0.22\n0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
262 -> 263 ;
264 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
263 -> 264 ;
265 [label="X[483] <= 0.01\nentropy = 1.17\nsamples = 1.4%\nvalue = [0.78, 0.0, 0.03, 0.08, 0.08, 0.0, 0.0, 0.0, 0.03\n0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
263 -> 265 ;
266 [label="X[298] <= 0.04\nentropy = 0.42\nsamples = 1.2%\nvalue = [0.93, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
265 -> 266 ;
267 [label="entropy = 0.0\nsamples = 1.0%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
266 -> 267 ;
268 [label="X[382] <= 0.04\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
266 -> 268 ;
269 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
268 -> 269 ;
270 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
268 -> 270 ;
271 [label="X[868] <= -0.01\nentropy = 1.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
265 -> 271 ;
272 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
271 -> 272 ;
273 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
271 -> 273 ;
274 [label="X[41] <= 0.04\nentropy = 2.34\nsamples = 0.9%\nvalue = [0.08, 0.0, 0.0, 0.0, 0.12, 0.0, 0.0, 0.0, 0.08, 0.04\n0.35, 0.31, 0.0, 0.04, 0.0]\nclass = Talking Points Memo"] ;
262 -> 274 ;
275 [label="X[346] <= -0.0\nentropy = 1.55\nsamples = 0.5%\nvalue = [0.15, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15, 0.08\n0.0, 0.62, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
274 -> 275 ;
276 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
275 -> 276 ;
277 [label="X[403] <= -0.01\nentropy = 1.52\nsamples = 0.3%\nvalue = [0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.2\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
275 -> 277 ;
278 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
277 -> 278 ;
279 [label="X[350] <= 0.01\nentropy = 0.92\nsamples = 0.2%\nvalue = [0.67, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
277 -> 279 ;
280 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
279 -> 280 ;
281 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
279 -> 281 ;
282 [label="X[152] <= -0.03\nentropy = 1.14\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.23, 0.0, 0.0, 0.0, 0.0, 0.0\n0.69, 0.0, 0.0, 0.08, 0.0]\nclass = Talking Points Memo"] ;
274 -> 282 ;
283 [label="X[518] <= -0.01\nentropy = 0.81\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.25, 0.0]\nclass = Buzzfeed News"] ;
282 -> 283 ;
284 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
283 -> 284 ;
285 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
283 -> 285 ;
286 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
282 -> 286 ;
287 [label="X[127] <= -0.01\nentropy = 2.52\nsamples = 3.6%\nvalue = [0.16, 0.04, 0.01, 0.0, 0.1, 0.04, 0.0, 0.0, 0.4, 0.0\n0.0, 0.04, 0.01, 0.02, 0.19]\nclass = New York Post"] ;
261 -> 287 ;
288 [label="X[2] <= -0.04\nentropy = 1.43\nsamples = 1.3%\nvalue = [0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.65, 0.0\n0.0, 0.0, 0.0, 0.05, 0.19]\nclass = New York Post"] ;
287 -> 288 ;
289 [label="X[395] <= 0.02\nentropy = 0.77\nsamples = 1.0%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.77, 0.0\n0.0, 0.0, 0.0, 0.0, 0.23]\nclass = New York Post"] ;
288 -> 289 ;
290 [label="X[331] <= 0.05\nentropy = 0.24\nsamples = 0.8%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96, 0.0\n0.0, 0.0, 0.0, 0.0, 0.04]\nclass = New York Post"] ;
289 -> 290 ;
291 [label="entropy = 0.0\nsamples = 0.7%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
290 -> 291 ;
292 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
290 -> 292 ;
293 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
289 -> 293 ;
294 [label="X[289] <= 0.01\nentropy = 0.92\nsamples = 0.2%\nvalue = [0.0, 0.67, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.33, 0.0]\nclass = Washington Post"] ;
288 -> 294 ;
295 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
294 -> 295 ;
296 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
294 -> 296 ;
297 [label="X[264] <= 0.0\nentropy = 2.55\nsamples = 2.4%\nvalue = [0.24, 0.0, 0.02, 0.0, 0.15, 0.06, 0.0, 0.0, 0.26\n0.0, 0.0, 0.06, 0.02, 0.0, 0.2]\nclass = New York Post"] ;
287 -> 297 ;
298 [label="X[199] <= 0.0\nentropy = 2.19\nsamples = 1.5%\nvalue = [0.21, 0.0, 0.0, 0.0, 0.21, 0.1, 0.0, 0.0, 0.4, 0.0\n0.0, 0.02, 0.02, 0.0, 0.02]\nclass = New York Post"] ;
297 -> 298 ;
299 [label="X[512] <= 0.0\nentropy = 1.4\nsamples = 0.9%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.68, 0.0\n0.0, 0.04, 0.04, 0.0, 0.04]\nclass = New York Post"] ;
298 -> 299 ;
300 [label="X[55] <= -0.0\nentropy = 1.55\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.62, 0.0, 0.0, 0.0, 0.12, 0.0\n0.0, 0.12, 0.0, 0.0, 0.12]\nclass = Buzzfeed News"] ;
299 -> 300 ;
301 [label="X[62] <= -0.01\nentropy = 1.58\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33, 0.0\n0.0, 0.33, 0.0, 0.0, 0.33]\nclass = New York Post"] ;
300 -> 301 ;
302 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
301 -> 302 ;
303 [label="X[493] <= -0.03\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.5, 0.0, 0.0, 0.5]\nclass = Guardian"] ;
301 -> 303 ;
304 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
303 -> 304 ;
305 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
303 -> 305 ;
306 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
300 -> 306 ;
307 [label="X[349] <= -0.05\nentropy = 0.32\nsamples = 0.5%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.94, 0.0\n0.0, 0.0, 0.06, 0.0, 0.0]\nclass = New York Post"] ;
299 -> 307 ;
308 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
307 -> 308 ;
309 [label="entropy = 0.0\nsamples = 0.5%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
307 -> 309 ;
310 [label="X[356] <= -0.0\nentropy = 1.47\nsamples = 0.6%\nvalue = [0.53, 0.0, 0.0, 0.0, 0.24, 0.24, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
298 -> 310 ;
311 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
310 -> 311 ;
312 [label="X[437] <= -0.01\nentropy = 1.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
310 -> 312 ;
313 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
312 -> 313 ;
314 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
312 -> 314 ;
315 [label="X[291] <= -0.01\nentropy = 1.78\nsamples = 0.9%\nvalue = [0.29, 0.0, 0.04, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.12, 0.0, 0.0, 0.5]\nclass = Fox News"] ;
297 -> 315 ;
316 [label="entropy = 0.0\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
315 -> 316 ;
317 [label="X[124] <= 0.0\nentropy = 1.55\nsamples = 0.5%\nvalue = [0.58, 0.0, 0.08, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.25, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
315 -> 317 ;
318 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
317 -> 318 ;
319 [label="X[840] <= 0.0\nentropy = 1.37\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.6, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
317 -> 319 ;
320 [label="X[816] <= -0.02\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.5, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
319 -> 320 ;
321 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
320 -> 321 ;
322 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
320 -> 322 ;
323 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
319 -> 323 ;
324 [label="X[61] <= -0.01\nentropy = 3.51\nsamples = 7.2%\nvalue = [0.08, 0.08, 0.08, 0.02, 0.03, 0.11, 0.05, 0.03, 0.17\n0.02, 0.0, 0.09, 0.04, 0.01, 0.16]\nclass = New York Post"] ;
260 -> 324 ;
325 [label="X[138] <= 0.01\nentropy = 2.81\nsamples = 3.2%\nvalue = [0.02, 0.02, 0.02, 0.0, 0.05, 0.19, 0.05, 0.0, 0.29\n0.04, 0.0, 0.07, 0.01, 0.0, 0.23]\nclass = New York Post"] ;
324 -> 325 ;
326 [label="X[469] <= 0.01\nentropy = 2.42\nsamples = 1.6%\nvalue = [0.02, 0.02, 0.02, 0.0, 0.06, 0.12, 0.0, 0.0, 0.5\n0.06, 0.0, 0.12, 0.02, 0.0, 0.06]\nclass = New York Post"] ;
325 -> 326 ;
327 [label="X[901] <= 0.03\nentropy = 0.88\nsamples = 0.9%\nvalue = [0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.86, 0.0\n0.0, 0.04, 0.04, 0.0, 0.04]\nclass = New York Post"] ;
326 -> 327 ;
328 [label="entropy = 0.0\nsamples = 0.7%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
327 -> 328 ;
329 [label="X[361] <= -0.01\nentropy = 2.0\nsamples = 0.2%\nvalue = [0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.25, 0.25, 0.0, 0.25]\nclass = Breitbart"] ;
327 -> 329 ;
330 [label="X[604] <= 0.02\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.5]\nclass = Breitbart"] ;
329 -> 330 ;
331 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
330 -> 331 ;
332 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
330 -> 332 ;
333 [label="X[949] <= -0.02\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.5, 0.5, 0.0, 0.0]\nclass = Guardian"] ;
329 -> 333 ;
334 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
333 -> 334 ;
335 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
333 -> 335 ;
336 [label="X[783] <= -0.01\nentropy = 2.7\nsamples = 0.7%\nvalue = [0.0, 0.05, 0.05, 0.0, 0.14, 0.27, 0.0, 0.0, 0.05\n0.14, 0.0, 0.23, 0.0, 0.0, 0.09]\nclass = NPR"] ;
326 -> 336 ;
337 [label="X[584] <= -0.01\nentropy = 1.62\nsamples = 0.3%\nvalue = [0.0, 0.09, 0.09, 0.0, 0.27, 0.55, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
336 -> 337 ;
338 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
337 -> 338 ;
339 [label="X[877] <= 0.05\nentropy = 1.37\nsamples = 0.2%\nvalue = [0.0, 0.2, 0.2, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
337 -> 339 ;
340 [label="X[737] <= 0.01\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
339 -> 340 ;
341 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
340 -> 341 ;
342 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
340 -> 342 ;
343 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
339 -> 343 ;
344 [label="X[556] <= -0.01\nentropy = 1.79\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09, 0.27\n0.0, 0.45, 0.0, 0.0, 0.18]\nclass = Guardian"] ;
336 -> 344 ;
345 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
344 -> 345 ;
346 [label="X[251] <= -0.03\nentropy = 1.46\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17, 0.5\n0.0, 0.0, 0.0, 0.0, 0.33]\nclass = Vox"] ;
344 -> 346 ;
347 [label="X[363] <= 0.03\nentropy = 0.92\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33, 0.0\n0.0, 0.0, 0.0, 0.0, 0.67]\nclass = Fox News"] ;
346 -> 347 ;
348 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
347 -> 348 ;
349 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
347 -> 349 ;
350 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
346 -> 350 ;
351 [label="X[291] <= 0.01\nentropy = 2.27\nsamples = 1.6%\nvalue = [0.02, 0.02, 0.02, 0.0, 0.05, 0.27, 0.12, 0.0, 0.02\n0.02, 0.0, 0.0, 0.0, 0.0, 0.44]\nclass = Fox News"] ;
325 -> 351 ;
352 [label="X[766] <= -0.02\nentropy = 1.46\nsamples = 0.9%\nvalue = [0.04, 0.04, 0.0, 0.0, 0.0, 0.04, 0.04, 0.0, 0.04\n0.04, 0.0, 0.0, 0.0, 0.0, 0.75]\nclass = Fox News"] ;
351 -> 352 ;
353 [label="X[965] <= -0.01\nentropy = 2.32\nsamples = 0.3%\nvalue = [0.0, 0.2, 0.0, 0.0, 0.0, 0.2, 0.2, 0.0, 0.2, 0.2\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
352 -> 353 ;
354 [label="X[992] <= 0.01\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.5, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
353 -> 354 ;
355 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
354 -> 355 ;
356 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
354 -> 356 ;
357 [label="X[37] <= -0.02\nentropy = 1.58\nsamples = 0.2%\nvalue = [0.0, 0.33, 0.0, 0.0, 0.0, 0.33, 0.0, 0.0, 0.0, 0.33\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
353 -> 357 ;
358 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
357 -> 358 ;
359 [label="X[687] <= -0.01\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
357 -> 359 ;
360 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
359 -> 360 ;
361 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
359 -> 361 ;
362 [label="X[586] <= -0.04\nentropy = 0.3\nsamples = 0.7%\nvalue = [0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.95]\nclass = Fox News"] ;
352 -> 362 ;
363 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
362 -> 363 ;
364 [label="entropy = 0.0\nsamples = 0.6%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
362 -> 364 ;
365 [label="X[868] <= 0.0\nentropy = 1.55\nsamples = 0.6%\nvalue = [0.0, 0.0, 0.06, 0.0, 0.12, 0.59, 0.24, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
351 -> 365 ;
366 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
365 -> 366 ;
367 [label="X[315] <= -0.02\nentropy = 1.38\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.14, 0.0, 0.29, 0.0, 0.57, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
365 -> 367 ;
368 [label="X[533] <= -0.0\nentropy = 0.92\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.33, 0.0, 0.67, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
367 -> 368 ;
369 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
368 -> 369 ;
370 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
368 -> 370 ;
371 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
367 -> 371 ;
372 [label="X[42] <= -0.01\nentropy = 3.53\nsamples = 4.0%\nvalue = [0.13, 0.13, 0.14, 0.04, 0.01, 0.05, 0.05, 0.06, 0.07\n0.0, 0.01, 0.11, 0.07, 0.03, 0.11]\nclass = CNN"] ;
324 -> 372 ;
373 [label="X[12] <= 0.0\nentropy = 2.9\nsamples = 1.6%\nvalue = [0.22, 0.0, 0.15, 0.0, 0.0, 0.1, 0.12, 0.17, 0.02\n0.0, 0.02, 0.0, 0.15, 0.05, 0.0]\nclass = Breitbart"] ;
372 -> 373 ;
374 [label="X[784] <= -0.01\nentropy = 0.68\nsamples = 0.4%\nvalue = [0.82, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.18, 0.0]\nclass = Breitbart"] ;
373 -> 374 ;
375 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
374 -> 375 ;
376 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
374 -> 376 ;
377 [label="X[889] <= 0.01\nentropy = 2.56\nsamples = 1.2%\nvalue = [0.0, 0.0, 0.2, 0.0, 0.0, 0.13, 0.17, 0.23, 0.03, 0.0\n0.03, 0.0, 0.2, 0.0, 0.0]\nclass = Reuters"] ;
373 -> 377 ;
378 [label="X[3] <= 0.02\nentropy = 1.79\nsamples = 0.7%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26, 0.37, 0.05, 0.0\n0.0, 0.0, 0.32, 0.0, 0.0]\nclass = Reuters"] ;
377 -> 378 ;
379 [label="X[601] <= -0.01\nentropy = 0.99\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.45, 0.0, 0.0, 0.0\n0.0, 0.0, 0.55, 0.0, 0.0]\nclass = Atlantic"] ;
378 -> 379 ;
380 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
379 -> 380 ;
381 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
379 -> 381 ;
382 [label="X[180] <= 0.01\nentropy = 0.54\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.88, 0.12, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
378 -> 382 ;
383 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
382 -> 383 ;
384 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
382 -> 384 ;
385 [label="X[545] <= -0.0\nentropy = 1.32\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.55, 0.0, 0.0, 0.36, 0.0, 0.0, 0.0, 0.0\n0.09, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
377 -> 385 ;
386 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
385 -> 386 ;
387 [label="X[904] <= -0.03\nentropy = 0.72\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0\n0.2, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
385 -> 387 ;
388 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
387 -> 388 ;
389 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
387 -> 389 ;
390 [label="X[769] <= -0.02\nentropy = 3.11\nsamples = 2.5%\nvalue = [0.07, 0.2, 0.13, 0.06, 0.01, 0.03, 0.01, 0.0, 0.1\n0.0, 0.0, 0.17, 0.03, 0.01, 0.17]\nclass = Washington Post"] ;
372 -> 390 ;
391 [label="X[323] <= 0.01\nentropy = 2.41\nsamples = 0.5%\nvalue = [0.36, 0.0, 0.0, 0.29, 0.07, 0.07, 0.07, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.07, 0.07]\nclass = Breitbart"] ;
390 -> 391 ;
392 [label="X[843] <= -0.01\nentropy = 1.66\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.57, 0.0, 0.14, 0.14, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.14]\nclass = Business Insider"] ;
391 -> 392 ;
393 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
392 -> 393 ;
394 [label="X[407] <= 0.01\nentropy = 1.58\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.33, 0.33, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.33]\nclass = NPR"] ;
392 -> 394 ;
395 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
394 -> 395 ;
396 [label="X[984] <= -0.01\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.5]\nclass = NPR"] ;
394 -> 396 ;
397 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
396 -> 397 ;
398 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
396 -> 398 ;
399 [label="X[973] <= -0.01\nentropy = 1.15\nsamples = 0.2%\nvalue = [0.71, 0.0, 0.0, 0.0, 0.14, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.14, 0.0]\nclass = Breitbart"] ;
391 -> 399 ;
400 [label="X[409] <= 0.03\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.5, 0.0]\nclass = Buzzfeed News"] ;
399 -> 400 ;
401 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
400 -> 401 ;
402 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
400 -> 402 ;
403 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
399 -> 403 ;
404 [label="X[32] <= 0.05\nentropy = 2.51\nsamples = 2.0%\nvalue = [0.0, 0.25, 0.16, 0.0, 0.0, 0.02, 0.0, 0.0, 0.12, 0.0\n0.0, 0.21, 0.04, 0.0, 0.2]\nclass = Washington Post"] ;
390 -> 404 ;
405 [label="X[946] <= -0.0\nentropy = 2.28\nsamples = 1.3%\nvalue = [0.0, 0.0, 0.16, 0.0, 0.0, 0.03, 0.0, 0.0, 0.19, 0.0\n0.0, 0.3, 0.05, 0.0, 0.27]\nclass = Guardian"] ;
404 -> 405 ;
406 [label="X[257] <= 0.01\nentropy = 1.5\nsamples = 0.7%\nvalue = [0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0\n0.0, 0.0, 0.0, 0.0, 0.5]\nclass = Fox News"] ;
405 -> 406 ;
407 [label="X[424] <= 0.01\nentropy = 1.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
406 -> 407 ;
408 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
407 -> 408 ;
409 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
407 -> 409 ;
410 [label="entropy = 0.0\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
406 -> 410 ;
411 [label="X[179] <= -0.02\nentropy = 1.61\nsamples = 0.6%\nvalue = [0.0, 0.0, 0.06, 0.0, 0.0, 0.06, 0.0, 0.0, 0.12, 0.0\n0.0, 0.65, 0.12, 0.0, 0.0]\nclass = Guardian"] ;
405 -> 411 ;
412 [label="X[303] <= 0.02\nentropy = 1.92\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.17, 0.0, 0.0, 0.17, 0.0, 0.0, 0.33, 0.0\n0.0, 0.0, 0.33, 0.0, 0.0]\nclass = New York Post"] ;
411 -> 412 ;
413 [label="X[205] <= -0.03\nentropy = 0.92\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.33, 0.0, 0.0, 0.0, 0.0, 0.0, 0.67, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
412 -> 413 ;
414 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
413 -> 414 ;
415 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
413 -> 415 ;
416 [label="X[700] <= -0.01\nentropy = 0.92\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.33, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.67, 0.0, 0.0]\nclass = Atlantic"] ;
412 -> 416 ;
417 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
416 -> 417 ;
418 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
416 -> 418 ;
419 [label="entropy = 0.0\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
411 -> 419 ;
420 [label="X[63] <= 0.02\nentropy = 1.19\nsamples = 0.7%\nvalue = [0.0, 0.74, 0.16, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.05, 0.0, 0.0, 0.05]\nclass = Washington Post"] ;
404 -> 420 ;
421 [label="X[309] <= -0.05\nentropy = 0.35\nsamples = 0.5%\nvalue = [0.0, 0.93, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.07, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
420 -> 421 ;
422 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
421 -> 422 ;
423 [label="entropy = 0.0\nsamples = 0.4%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
421 -> 423 ;
424 [label="X[363] <= -0.04\nentropy = 0.81\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.25]\nclass = CNN"] ;
420 -> 424 ;
425 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
424 -> 425 ;
426 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
424 -> 426 ;
427 [label="X[919] <= 0.01\nentropy = 3.66\nsamples = 8.3%\nvalue = [0.04, 0.04, 0.16, 0.01, 0.11, 0.09, 0.05, 0.03, 0.02\n0.06, 0.04, 0.07, 0.08, 0.1, 0.1]\nclass = CNN"] ;
259 -> 427 ;
428 [label="X[797] <= 0.0\nentropy = 3.48\nsamples = 5.0%\nvalue = [0.0, 0.06, 0.11, 0.01, 0.11, 0.04, 0.07, 0.02, 0.0\n0.07, 0.07, 0.1, 0.08, 0.16, 0.11]\nclass = National Review"] ;
427 -> 428 ;
429 [label="X[452] <= 0.01\nentropy = 3.21\nsamples = 2.6%\nvalue = [0.0, 0.06, 0.16, 0.0, 0.12, 0.06, 0.02, 0.04, 0.0\n0.07, 0.02, 0.12, 0.15, 0.0, 0.17]\nclass = Fox News"] ;
428 -> 429 ;
430 [label="X[7] <= 0.0\nentropy = 2.67\nsamples = 1.7%\nvalue = [0.0, 0.11, 0.24, 0.0, 0.02, 0.11, 0.04, 0.04, 0.0\n0.0, 0.0, 0.22, 0.2, 0.0, 0.0]\nclass = CNN"] ;
429 -> 430 ;
431 [label="X[401] <= 0.01\nentropy = 1.8\nsamples = 0.9%\nvalue = [0.0, 0.19, 0.38, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0\n0.0, 0.35, 0.0, 0.0, 0.0]\nclass = CNN"] ;
430 -> 431 ;
432 [label="X[173] <= -0.01\nentropy = 1.0\nsamples = 0.6%\nvalue = [0.0, 0.0, 0.53, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.47, 0.0, 0.0, 0.0]\nclass = CNN"] ;
431 -> 432 ;
433 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
432 -> 433 ;
434 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
432 -> 434 ;
435 [label="X[934] <= -0.02\nentropy = 0.86\nsamples = 0.3%\nvalue = [0.0, 0.71, 0.0, 0.0, 0.0, 0.0, 0.0, 0.29, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
431 -> 435 ;
436 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
435 -> 436 ;
437 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
435 -> 437 ;
438 [label="X[102] <= 0.0\nentropy = 2.03\nsamples = 0.8%\nvalue = [0.0, 0.0, 0.05, 0.0, 0.05, 0.26, 0.11, 0.0, 0.0, 0.0\n0.0, 0.05, 0.47, 0.0, 0.0]\nclass = Atlantic"] ;
430 -> 438 ;
439 [label="X[674] <= -0.0\nentropy = 1.66\nsamples = 0.5%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.11, 0.56, 0.22, 0.0, 0.0, 0.0\n0.0, 0.11, 0.0, 0.0, 0.0]\nclass = NPR"] ;
438 -> 439 ;
440 [label="X[700] <= 0.01\nentropy = 1.5\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.5, 0.0, 0.0, 0.0\n0.0, 0.25, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
439 -> 440 ;
441 [label="X[136] <= -0.01\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.5, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
440 -> 441 ;
442 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
441 -> 442 ;
443 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
441 -> 443 ;
444 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
440 -> 444 ;
445 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
439 -> 445 ;
446 [label="X[644] <= 0.03\nentropy = 0.47\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.9, 0.0, 0.0]\nclass = Atlantic"] ;
438 -> 446 ;
447 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
446 -> 447 ;
448 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
446 -> 448 ;
449 [label="X[774] <= 0.0\nentropy = 2.34\nsamples = 0.9%\nvalue = [0.0, 0.0, 0.05, 0.0, 0.24, 0.0, 0.0, 0.03, 0.0, 0.16\n0.05, 0.0, 0.08, 0.0, 0.38]\nclass = Fox News"] ;
429 -> 449 ;
450 [label="X[165] <= 0.02\nentropy = 1.97\nsamples = 0.6%\nvalue = [0.0, 0.0, 0.05, 0.0, 0.43, 0.0, 0.0, 0.0, 0.0, 0.29\n0.1, 0.0, 0.14, 0.0, 0.0]\nclass = Buzzfeed News"] ;
449 -> 450 ;
451 [label="X[208] <= 0.0\nentropy = 1.44\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.55\n0.18, 0.0, 0.27, 0.0, 0.0]\nclass = Vox"] ;
450 -> 451 ;
452 [label="X[302] <= 0.02\nentropy = 0.97\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.4, 0.0, 0.6, 0.0, 0.0]\nclass = Atlantic"] ;
451 -> 452 ;
453 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
452 -> 453 ;
454 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
452 -> 454 ;
455 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
451 -> 455 ;
456 [label="X[824] <= -0.0\nentropy = 0.47\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.1, 0.0, 0.9, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
450 -> 456 ;
457 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
456 -> 457 ;
458 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
456 -> 458 ;
459 [label="X[718] <= 0.01\nentropy = 0.67\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.88]\nclass = Fox News"] ;
449 -> 459 ;
460 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
459 -> 460 ;
461 [label="X[574] <= -0.02\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
459 -> 461 ;
462 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
461 -> 462 ;
463 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
461 -> 463 ;
464 [label="X[154] <= 0.01\nentropy = 2.92\nsamples = 2.4%\nvalue = [0.0, 0.06, 0.06, 0.01, 0.1, 0.01, 0.13, 0.0, 0.0\n0.06, 0.13, 0.07, 0.0, 0.34, 0.03]\nclass = National Review"] ;
428 -> 464 ;
465 [label="X[928] <= -0.0\nentropy = 2.83\nsamples = 1.2%\nvalue = [0.0, 0.06, 0.09, 0.03, 0.18, 0.03, 0.24, 0.0, 0.0\n0.0, 0.26, 0.06, 0.0, 0.03, 0.03]\nclass = Talking Points Memo"] ;
464 -> 465 ;
466 [label="X[434] <= 0.01\nentropy = 1.88\nsamples = 0.7%\nvalue = [0.0, 0.05, 0.15, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0\n0.45, 0.0, 0.0, 0.0, 0.05]\nclass = Talking Points Memo"] ;
465 -> 466 ;
467 [label="X[580] <= -0.01\nentropy = 0.47\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.9, 0.0, 0.0, 0.0, 0.1]\nclass = Talking Points Memo"] ;
466 -> 467 ;
468 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
467 -> 468 ;
469 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
467 -> 469 ;
470 [label="X[774] <= 0.01\nentropy = 1.3\nsamples = 0.4%\nvalue = [0.0, 0.1, 0.3, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
466 -> 470 ;
471 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
470 -> 471 ;
472 [label="X[773] <= -0.01\nentropy = 0.81\nsamples = 0.2%\nvalue = [0.0, 0.25, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
470 -> 472 ;
473 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
472 -> 473 ;
474 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
472 -> 474 ;
475 [label="X[484] <= -0.01\nentropy = 1.95\nsamples = 0.5%\nvalue = [0.0, 0.07, 0.0, 0.07, 0.0, 0.07, 0.57, 0.0, 0.0, 0.0\n0.0, 0.14, 0.0, 0.07, 0.0]\nclass = New York Times"] ;
465 -> 475 ;
476 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
475 -> 476 ;
477 [label="X[84] <= -0.01\nentropy = 2.25\nsamples = 0.3%\nvalue = [0.0, 0.17, 0.0, 0.17, 0.0, 0.17, 0.0, 0.0, 0.0, 0.0\n0.0, 0.33, 0.0, 0.17, 0.0]\nclass = Guardian"] ;
475 -> 477 ;
478 [label="X[889] <= -0.0\nentropy = 1.58\nsamples = 0.2%\nvalue = [0.0, 0.33, 0.0, 0.33, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.33, 0.0]\nclass = Washington Post"] ;
477 -> 478 ;
479 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
478 -> 479 ;
480 [label="X[924] <= 0.0\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.5, 0.0]\nclass = Washington Post"] ;
478 -> 480 ;
481 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
480 -> 481 ;
482 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
480 -> 482 ;
483 [label="X[268] <= 0.03\nentropy = 0.92\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.33, 0.0, 0.0, 0.0, 0.0\n0.0, 0.67, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
477 -> 483 ;
484 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
483 -> 484 ;
485 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
483 -> 485 ;
486 [label="X[614] <= 0.0\nentropy = 1.87\nsamples = 1.2%\nvalue = [0.0, 0.06, 0.03, 0.0, 0.03, 0.0, 0.03, 0.0, 0.0\n0.11, 0.0, 0.08, 0.0, 0.64, 0.03]\nclass = National Review"] ;
464 -> 486 ;
487 [label="X[172] <= 0.02\nentropy = 0.92\nsamples = 0.8%\nvalue = [0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04\n0.0, 0.11, 0.0, 0.82, 0.0]\nclass = National Review"] ;
486 -> 487 ;
488 [label="X[391] <= 0.03\nentropy = 0.25\nsamples = 0.6%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.04, 0.0, 0.96, 0.0]\nclass = National Review"] ;
487 -> 488 ;
489 [label="entropy = 0.0\nsamples = 0.6%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
488 -> 489 ;
490 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
488 -> 490 ;
491 [label="X[897] <= -0.01\nentropy = 1.5\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25\n0.0, 0.5, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
487 -> 491 ;
492 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
491 -> 492 ;
493 [label="X[12] <= -0.06\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
491 -> 493 ;
494 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
493 -> 494 ;
495 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
493 -> 495 ;
496 [label="X[534] <= 0.0\nentropy = 2.16\nsamples = 0.4%\nvalue = [0.0, 0.25, 0.0, 0.0, 0.12, 0.0, 0.12, 0.0, 0.0, 0.38\n0.0, 0.0, 0.0, 0.0, 0.12]\nclass = Vox"] ;
486 -> 496 ;
497 [label="X[446] <= -0.02\nentropy = 0.81\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.75\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
496 -> 497 ;
498 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
497 -> 498 ;
499 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
497 -> 499 ;
500 [label="X[276] <= -0.03\nentropy = 1.5\nsamples = 0.2%\nvalue = [0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.25]\nclass = Washington Post"] ;
496 -> 500 ;
501 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
500 -> 501 ;
502 [label="X[453] <= 0.01\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.5]\nclass = New York Times"] ;
500 -> 502 ;
503 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
502 -> 503 ;
504 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
502 -> 504 ;
505 [label="X[254] <= -0.03\nentropy = 3.25\nsamples = 3.2%\nvalue = [0.09, 0.0, 0.24, 0.02, 0.1, 0.18, 0.02, 0.05, 0.06\n0.05, 0.0, 0.02, 0.08, 0.01, 0.1]\nclass = CNN"] ;
427 -> 505 ;
506 [label="X[991] <= -0.01\nentropy = 1.46\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.15, 0.0, 0.0, 0.0, 0.0, 0.46, 0.38\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
505 -> 506 ;
507 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
506 -> 507 ;
508 [label="X[911] <= -0.01\nentropy = 0.86\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.29, 0.0, 0.0, 0.0, 0.0, 0.0, 0.71\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
506 -> 508 ;
509 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
508 -> 509 ;
510 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
508 -> 510 ;
511 [label="X[397] <= 0.01\nentropy = 2.88\nsamples = 2.9%\nvalue = [0.1, 0.0, 0.27, 0.0, 0.11, 0.2, 0.02, 0.06, 0.0, 0.0\n0.0, 0.02, 0.09, 0.01, 0.11]\nclass = CNN"] ;
505 -> 511 ;
512 [label="X[997] <= 0.01\nentropy = 2.23\nsamples = 1.6%\nvalue = [0.08, 0.0, 0.29, 0.0, 0.02, 0.35, 0.04, 0.0, 0.0\n0.0, 0.0, 0.02, 0.0, 0.0, 0.2]\nclass = NPR"] ;
511 -> 512 ;
513 [label="X[954] <= 0.0\nentropy = 1.77\nsamples = 1.2%\nvalue = [0.12, 0.0, 0.06, 0.0, 0.03, 0.52, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.27]\nclass = NPR"] ;
512 -> 513 ;
514 [label="X[609] <= 0.0\nentropy = 1.55\nsamples = 0.5%\nvalue = [0.15, 0.0, 0.15, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.62]\nclass = Fox News"] ;
513 -> 514 ;
515 [label="X[538] <= 0.0\nentropy = 1.52\nsamples = 0.3%\nvalue = [0.4, 0.0, 0.4, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
514 -> 515 ;
516 [label="X[949] <= -0.0\nentropy = 0.92\nsamples = 0.2%\nvalue = [0.67, 0.0, 0.0, 0.0, 0.33, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
515 -> 516 ;
517 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
516 -> 517 ;
518 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
516 -> 518 ;
519 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
515 -> 519 ;
520 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
514 -> 520 ;
521 [label="X[373] <= 0.02\nentropy = 0.75\nsamples = 0.7%\nvalue = [0.1, 0.0, 0.0, 0.0, 0.0, 0.85, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.05]\nclass = NPR"] ;
513 -> 521 ;
522 [label="entropy = 0.0\nsamples = 0.6%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
521 -> 522 ;
523 [label="X[1] <= -0.16\nentropy = 0.92\nsamples = 0.1%\nvalue = [0.67, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.33]\nclass = Breitbart"] ;
521 -> 523 ;
524 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
523 -> 524 ;
525 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
523 -> 525 ;
526 [label="X[998] <= -0.0\nentropy = 1.19\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.12, 0.0, 0.0, 0.0\n0.0, 0.06, 0.0, 0.0, 0.06]\nclass = CNN"] ;
512 -> 526 ;
527 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
526 -> 527 ;
528 [label="X[369] <= -0.02\nentropy = 1.5\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0\n0.0, 0.25, 0.0, 0.0, 0.25]\nclass = New York Times"] ;
526 -> 528 ;
529 [label="X[845] <= -0.02\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.5, 0.0, 0.0, 0.5]\nclass = Guardian"] ;
528 -> 529 ;
530 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
529 -> 530 ;
531 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
529 -> 531 ;
532 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
528 -> 532 ;
533 [label="X[537] <= -0.0\nentropy = 2.6\nsamples = 1.3%\nvalue = [0.12, 0.0, 0.25, 0.0, 0.22, 0.02, 0.0, 0.12, 0.0\n0.0, 0.0, 0.02, 0.2, 0.02, 0.0]\nclass = CNN"] ;
511 -> 533 ;
534 [label="X[910] <= -0.0\nentropy = 1.28\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.09, 0.09, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.73, 0.09, 0.0]\nclass = Atlantic"] ;
533 -> 534 ;
535 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
534 -> 535 ;
536 [label="X[747] <= -0.02\nentropy = 1.58\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.33, 0.33, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.33, 0.0]\nclass = Buzzfeed News"] ;
534 -> 536 ;
537 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
536 -> 537 ;
538 [label="X[25] <= 0.02\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.5, 0.0]\nclass = NPR"] ;
536 -> 538 ;
539 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
538 -> 539 ;
540 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
538 -> 540 ;
541 [label="X[686] <= -0.02\nentropy = 2.08\nsamples = 0.9%\nvalue = [0.17, 0.0, 0.34, 0.0, 0.28, 0.0, 0.0, 0.17, 0.0, 0.0\n0.0, 0.03, 0.0, 0.0, 0.0]\nclass = CNN"] ;
533 -> 541 ;
542 [label="X[696] <= -0.01\nentropy = 0.5\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.89, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.11, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
541 -> 542 ;
543 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
542 -> 543 ;
544 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
542 -> 544 ;
545 [label="X[784] <= 0.0\nentropy = 1.5\nsamples = 0.7%\nvalue = [0.25, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
541 -> 545 ;
546 [label="X[679] <= -0.01\nentropy = 0.44\nsamples = 0.4%\nvalue = [0.09, 0.0, 0.91, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
545 -> 546 ;
547 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
546 -> 547 ;
548 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
546 -> 548 ;
549 [label="X[919] <= 0.01\nentropy = 0.99\nsamples = 0.3%\nvalue = [0.44, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.56, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
545 -> 549 ;
550 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
549 -> 550 ;
551 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
549 -> 551 ;
552 [label="X[45] <= -0.03\nentropy = 3.52\nsamples = 16.4%\nvalue = [0.04, 0.08, 0.05, 0.1, 0.07, 0.03, 0.08, 0.25, 0.05\n0.02, 0.01, 0.05, 0.04, 0.03, 0.11]\nclass = Reuters"] ;
2 -> 552 ;
553 [label="X[74] <= 0.02\nentropy = 2.24\nsamples = 4.6%\nvalue = [0.01, 0.0, 0.0, 0.07, 0.03, 0.0, 0.01, 0.58, 0.08\n0.02, 0.0, 0.07, 0.01, 0.02, 0.07]\nclass = Reuters"] ;
552 -> 553 ;
554 [label="X[367] <= -0.02\nentropy = 1.64\nsamples = 3.5%\nvalue = [0.01, 0.0, 0.0, 0.1, 0.02, 0.0, 0.02, 0.68, 0.11\n0.0, 0.0, 0.05, 0.01, 0.0, 0.0]\nclass = Reuters"] ;
553 -> 554 ;
555 [label="X[168] <= -0.02\nentropy = 1.58\nsamples = 1.1%\nvalue = [0.0, 0.0, 0.0, 0.34, 0.0, 0.0, 0.0, 0.34, 0.31, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
554 -> 555 ;
556 [label="X[3] <= -0.14\nentropy = 0.44\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.91, 0.0, 0.0, 0.0, 0.09, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
555 -> 556 ;
557 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
556 -> 557 ;
558 [label="entropy = 0.0\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
556 -> 558 ;
559 [label="X[23] <= 0.02\nentropy = 1.0\nsamples = 0.7%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
555 -> 559 ;
560 [label="X[559] <= -0.01\nentropy = 0.47\nsamples = 0.5%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9, 0.1, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
559 -> 560 ;
561 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
560 -> 561 ;
562 [label="entropy = 0.0\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
560 -> 562 ;
563 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
559 -> 563 ;
564 [label="X[396] <= -0.03\nentropy = 1.12\nsamples = 2.4%\nvalue = [0.01, 0.0, 0.0, 0.0, 0.03, 0.0, 0.03, 0.82, 0.03\n0.0, 0.0, 0.07, 0.01, 0.0, 0.0]\nclass = Reuters"] ;
554 -> 564 ;
565 [label="X[72] <= -0.01\nentropy = 1.56\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.29, 0.0, 0.0, 0.0, 0.29, 0.0\n0.0, 0.43, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
564 -> 565 ;
566 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
565 -> 566 ;
567 [label="X[960] <= 0.02\nentropy = 1.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.5, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
565 -> 567 ;
568 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
567 -> 568 ;
569 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
567 -> 569 ;
570 [label="X[913] <= 0.03\nentropy = 0.63\nsamples = 2.0%\nvalue = [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.91, 0.0, 0.0\n0.0, 0.03, 0.02, 0.0, 0.0]\nclass = Reuters"] ;
564 -> 570 ;
571 [label="X[126] <= 0.09\nentropy = 0.24\nsamples = 1.9%\nvalue = [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.97, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
570 -> 571 ;
572 [label="entropy = 0.0\nsamples = 1.8%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
571 -> 572 ;
573 [label="X[569] <= -0.0\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
571 -> 573 ;
574 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
573 -> 574 ;
575 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
573 -> 575 ;
576 [label="X[426] <= -0.0\nentropy = 1.5\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0\n0.0, 0.5, 0.25, 0.0, 0.0]\nclass = Guardian"] ;
570 -> 576 ;
577 [label="X[37] <= 0.01\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0\n0.0, 0.0, 0.5, 0.0, 0.0]\nclass = New York Times"] ;
576 -> 577 ;
578 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
577 -> 578 ;
579 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
577 -> 579 ;
580 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
576 -> 580 ;
581 [label="X[52] <= 0.01\nentropy = 2.59\nsamples = 1.2%\nvalue = [0.03, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.29, 0.0, 0.09\n0.0, 0.15, 0.03, 0.09, 0.26]\nclass = Reuters"] ;
553 -> 581 ;
582 [label="X[236] <= 0.01\nentropy = 1.23\nsamples = 0.7%\nvalue = [0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.45]\nclass = Reuters"] ;
581 -> 582 ;
583 [label="X[612] <= -0.03\nentropy = 0.47\nsamples = 0.3%\nvalue = [0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.9]\nclass = Fox News"] ;
582 -> 583 ;
584 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
583 -> 584 ;
585 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
583 -> 585 ;
586 [label="entropy = 0.0\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
582 -> 586 ;
587 [label="X[20] <= 0.0\nentropy = 2.16\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.14, 0.0, 0.0, 0.0, 0.0, 0.21\n0.0, 0.36, 0.07, 0.21, 0.0]\nclass = Guardian"] ;
581 -> 587 ;
588 [label="X[645] <= 0.0\nentropy = 1.45\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43\n0.0, 0.0, 0.14, 0.43, 0.0]\nclass = Vox"] ;
587 -> 588 ;
589 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
588 -> 589 ;
590 [label="X[953] <= -0.01\nentropy = 0.81\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.25, 0.75, 0.0]\nclass = National Review"] ;
588 -> 590 ;
591 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
590 -> 591 ;
592 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
590 -> 592 ;
593 [label="X[973] <= 0.01\nentropy = 0.86\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.29, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.71, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
587 -> 593 ;
594 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
593 -> 594 ;
595 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
593 -> 595 ;
596 [label="X[58] <= -0.01\nentropy = 3.68\nsamples = 11.8%\nvalue = [0.04, 0.1, 0.07, 0.11, 0.08, 0.05, 0.1, 0.13, 0.04\n0.02, 0.01, 0.04, 0.05, 0.03, 0.12]\nclass = Reuters"] ;
552 -> 596 ;
597 [label="X[221] <= -0.01\nentropy = 3.62\nsamples = 4.5%\nvalue = [0.01, 0.02, 0.1, 0.1, 0.02, 0.12, 0.12, 0.07, 0.04\n0.05, 0.01, 0.09, 0.06, 0.06, 0.12]\nclass = Fox News"] ;
596 -> 597 ;
598 [label="X[35] <= -0.01\nentropy = 2.76\nsamples = 1.7%\nvalue = [0.0, 0.0, 0.0, 0.05, 0.03, 0.05, 0.29, 0.05, 0.0\n0.05, 0.0, 0.03, 0.03, 0.1, 0.29]\nclass = New York Times"] ;
597 -> 598 ;
599 [label="X[468] <= -0.0\nentropy = 1.78\nsamples = 0.8%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.07, 0.57, 0.0, 0.0, 0.1\n0.0, 0.0, 0.07, 0.2, 0.0]\nclass = New York Times"] ;
598 -> 599 ;
600 [label="entropy = 0.0\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
599 -> 600 ;
601 [label="X[339] <= -0.01\nentropy = 1.83\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.15, 0.0, 0.0, 0.0, 0.23\n0.0, 0.0, 0.15, 0.46, 0.0]\nclass = National Review"] ;
599 -> 601 ;
602 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
601 -> 602 ;
603 [label="X[922] <= 0.0\nentropy = 1.56\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.29, 0.0, 0.0, 0.0, 0.43\n0.0, 0.0, 0.29, 0.0, 0.0]\nclass = Vox"] ;
601 -> 603 ;
604 [label="X[447] <= 0.02\nentropy = 1.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.5, 0.0, 0.0]\nclass = NPR"] ;
603 -> 604 ;
605 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
604 -> 605 ;
606 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
604 -> 606 ;
607 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
603 -> 607 ;
608 [label="X[241] <= 0.03\nentropy = 1.84\nsamples = 0.9%\nvalue = [0.0, 0.0, 0.0, 0.11, 0.07, 0.04, 0.0, 0.11, 0.0, 0.0\n0.0, 0.07, 0.0, 0.0, 0.61]\nclass = Fox News"] ;
598 -> 608 ;
609 [label="X[406] <= -0.01\nentropy = 0.75\nsamples = 0.7%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0\n0.0, 0.1, 0.0, 0.0, 0.85]\nclass = Fox News"] ;
608 -> 609 ;
610 [label="X[619] <= 0.0\nentropy = 0.92\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.33, 0.0, 0.0, 0.0, 0.0\n0.0, 0.67, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
609 -> 610 ;
611 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
610 -> 611 ;
612 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
610 -> 612 ;
613 [label="entropy = 0.0\nsamples = 0.5%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
609 -> 613 ;
614 [label="X[985] <= -0.01\nentropy = 1.56\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.38, 0.25, 0.0, 0.0, 0.38, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
608 -> 614 ;
615 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
614 -> 615 ;
616 [label="X[731] <= 0.01\nentropy = 0.97\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.6, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
614 -> 616 ;
617 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
616 -> 617 ;
618 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
616 -> 618 ;
619 [label="X[948] <= -0.01\nentropy = 3.37\nsamples = 2.8%\nvalue = [0.02, 0.03, 0.17, 0.14, 0.01, 0.16, 0.0, 0.08, 0.07\n0.05, 0.01, 0.13, 0.07, 0.03, 0.01]\nclass = CNN"] ;
597 -> 619 ;
620 [label="X[678] <= -0.0\nentropy = 2.06\nsamples = 0.7%\nvalue = [0.0, 0.0, 0.18, 0.07, 0.0, 0.5, 0.0, 0.0, 0.07, 0.14\n0.0, 0.0, 0.0, 0.04, 0.0]\nclass = NPR"] ;
619 -> 620 ;
621 [label="X[871] <= -0.02\nentropy = 0.54\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.88, 0.0, 0.0, 0.12, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
620 -> 621 ;
622 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
621 -> 622 ;
623 [label="entropy = 0.0\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
621 -> 623 ;
624 [label="X[295] <= -0.01\nentropy = 1.78\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.42, 0.17, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33\n0.0, 0.0, 0.0, 0.08, 0.0]\nclass = CNN"] ;
620 -> 624 ;
625 [label="X[170] <= -0.0\nentropy = 0.65\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.83, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.17, 0.0]\nclass = CNN"] ;
624 -> 625 ;
626 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
625 -> 626 ;
627 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
625 -> 627 ;
628 [label="X[666] <= -0.03\nentropy = 0.92\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.33, 0.0, 0.0, 0.0, 0.0, 0.0, 0.67\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
624 -> 628 ;
629 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
628 -> 629 ;
630 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
628 -> 630 ;
631 [label="X[37] <= 0.03\nentropy = 3.16\nsamples = 2.1%\nvalue = [0.03, 0.05, 0.17, 0.17, 0.02, 0.0, 0.0, 0.12, 0.07\n0.0, 0.02, 0.19, 0.1, 0.03, 0.02]\nclass = Guardian"] ;
619 -> 631 ;
632 [label="X[760] <= -0.0\nentropy = 2.75\nsamples = 1.4%\nvalue = [0.0, 0.08, 0.21, 0.26, 0.03, 0.0, 0.0, 0.16, 0.0\n0.0, 0.03, 0.0, 0.16, 0.05, 0.03]\nclass = Business Insider"] ;
631 -> 632 ;
633 [label="X[522] <= 0.0\nentropy = 1.56\nsamples = 0.6%\nvalue = [0.0, 0.0, 0.0, 0.67, 0.0, 0.0, 0.0, 0.13, 0.0, 0.0\n0.07, 0.0, 0.0, 0.07, 0.07]\nclass = Business Insider"] ;
632 -> 633 ;
634 [label="entropy = 0.0\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
633 -> 634 ;
635 [label="X[798] <= 0.0\nentropy = 1.92\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0\n0.2, 0.0, 0.0, 0.2, 0.2]\nclass = Reuters"] ;
633 -> 635 ;
636 [label="X[95] <= -0.06\nentropy = 0.92\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.67, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.33]\nclass = Reuters"] ;
635 -> 636 ;
637 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
636 -> 637 ;
638 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
636 -> 638 ;
639 [label="X[47] <= 0.01\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.5, 0.0, 0.0, 0.5, 0.0]\nclass = Talking Points Memo"] ;
635 -> 639 ;
640 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
639 -> 640 ;
641 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
639 -> 641 ;
642 [label="X[349] <= -0.0\nentropy = 2.25\nsamples = 0.8%\nvalue = [0.0, 0.13, 0.35, 0.0, 0.04, 0.0, 0.0, 0.17, 0.0, 0.0\n0.0, 0.0, 0.26, 0.04, 0.0]\nclass = CNN"] ;
632 -> 642 ;
643 [label="X[383] <= 0.03\nentropy = 1.19\nsamples = 0.4%\nvalue = [0.0, 0.25, 0.67, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.08, 0.0]\nclass = CNN"] ;
642 -> 643 ;
644 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
643 -> 644 ;
645 [label="X[183] <= 0.03\nentropy = 0.81\nsamples = 0.2%\nvalue = [0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.25, 0.0]\nclass = Washington Post"] ;
643 -> 645 ;
646 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
645 -> 646 ;
647 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
645 -> 647 ;
648 [label="X[71] <= 0.02\nentropy = 1.32\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.09, 0.0, 0.0, 0.36, 0.0, 0.0\n0.0, 0.0, 0.55, 0.0, 0.0]\nclass = Atlantic"] ;
642 -> 648 ;
649 [label="X[869] <= 0.02\nentropy = 0.72\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.8, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
648 -> 649 ;
650 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
649 -> 650 ;
651 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
649 -> 651 ;
652 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
648 -> 652 ;
653 [label="X[144] <= -0.01\nentropy = 1.82\nsamples = 0.7%\nvalue = [0.1, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.05, 0.2, 0.0\n0.0, 0.55, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
631 -> 653 ;
654 [label="X[36] <= -0.04\nentropy = 1.84\nsamples = 0.4%\nvalue = [0.22, 0.0, 0.22, 0.0, 0.0, 0.0, 0.0, 0.11, 0.44, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
653 -> 654 ;
655 [label="X[54] <= 0.01\nentropy = 1.52\nsamples = 0.2%\nvalue = [0.4, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
654 -> 655 ;
656 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
655 -> 656 ;
657 [label="X[45] <= 0.09\nentropy = 0.92\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.67, 0.0, 0.0, 0.0, 0.0, 0.33, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
655 -> 657 ;
658 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
657 -> 658 ;
659 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
657 -> 659 ;
660 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
654 -> 660 ;
661 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
653 -> 661 ;
662 [label="X[6] <= -0.03\nentropy = 3.32\nsamples = 7.3%\nvalue = [0.06, 0.16, 0.05, 0.12, 0.12, 0.0, 0.09, 0.16, 0.05\n0.0, 0.02, 0.01, 0.05, 0.0, 0.12]\nclass = Reuters"] ;
596 -> 662 ;
663 [label="X[481] <= -0.0\nentropy = 2.85\nsamples = 2.9%\nvalue = [0.09, 0.11, 0.03, 0.26, 0.22, 0.0, 0.1, 0.12, 0.0\n0.0, 0.01, 0.02, 0.0, 0.0, 0.02]\nclass = Business Insider"] ;
662 -> 663 ;
664 [label="X[203] <= 0.0\nentropy = 2.46\nsamples = 1.4%\nvalue = [0.05, 0.26, 0.05, 0.36, 0.0, 0.0, 0.03, 0.15, 0.0\n0.0, 0.0, 0.05, 0.0, 0.0, 0.05]\nclass = Business Insider"] ;
663 -> 664 ;
665 [label="X[446] <= 0.0\nentropy = 2.55\nsamples = 0.5%\nvalue = [0.18, 0.0, 0.18, 0.0, 0.0, 0.0, 0.09, 0.18, 0.0, 0.0\n0.0, 0.18, 0.0, 0.0, 0.18]\nclass = Breitbart"] ;
664 -> 665 ;
666 [label="X[306] <= 0.02\nentropy = 1.52\nsamples = 0.2%\nvalue = [0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.4]\nclass = Breitbart"] ;
665 -> 666 ;
667 [label="X[977] <= -0.0\nentropy = 0.92\nsamples = 0.2%\nvalue = [0.67, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
666 -> 667 ;
668 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
667 -> 668 ;
669 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
667 -> 669 ;
670 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
666 -> 670 ;
671 [label="X[551] <= 0.02\nentropy = 1.58\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.33, 0.0, 0.0, 0.0, 0.0, 0.33, 0.0, 0.0\n0.0, 0.33, 0.0, 0.0, 0.0]\nclass = CNN"] ;
665 -> 671 ;
672 [label="X[504] <= 0.01\nentropy = 1.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0\n0.0, 0.5, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
671 -> 672 ;
673 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
672 -> 673 ;
674 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
672 -> 674 ;
675 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
671 -> 675 ;
676 [label="X[591] <= -0.0\nentropy = 1.43\nsamples = 0.9%\nvalue = [0.0, 0.36, 0.0, 0.5, 0.0, 0.0, 0.0, 0.14, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
664 -> 676 ;
677 [label="X[128] <= -0.03\nentropy = 1.09\nsamples = 0.5%\nvalue = [0.0, 0.71, 0.0, 0.07, 0.0, 0.0, 0.0, 0.21, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
676 -> 677 ;
678 [label="X[16] <= -0.02\nentropy = 0.81\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.75, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
677 -> 678 ;
679 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
678 -> 679 ;
680 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
678 -> 680 ;
681 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
677 -> 681 ;
682 [label="X[221] <= 0.06\nentropy = 0.37\nsamples = 0.5%\nvalue = [0.0, 0.0, 0.0, 0.93, 0.0, 0.0, 0.0, 0.07, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
676 -> 682 ;
683 [label="entropy = 0.0\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
682 -> 683 ;
684 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
682 -> 684 ;
685 [label="X[165] <= 0.0\nentropy = 2.32\nsamples = 1.5%\nvalue = [0.12, 0.0, 0.02, 0.18, 0.4, 0.0, 0.16, 0.1, 0.0, 0.0\n0.02, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
663 -> 685 ;
686 [label="X[947] <= -0.01\nentropy = 1.65\nsamples = 0.9%\nvalue = [0.18, 0.0, 0.0, 0.0, 0.61, 0.0, 0.06, 0.12, 0.0, 0.0\n0.03, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
685 -> 686 ;
687 [label="X[535] <= -0.01\nentropy = 1.74\nsamples = 0.4%\nvalue = [0.46, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15, 0.31, 0.0, 0.0\n0.08, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
686 -> 687 ;
688 [label="X[569] <= -0.03\nentropy = 0.92\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33, 0.67, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
687 -> 688 ;
689 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
688 -> 689 ;
690 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
688 -> 690 ;
691 [label="X[489] <= 0.01\nentropy = 0.59\nsamples = 0.2%\nvalue = [0.86, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.14, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
687 -> 691 ;
692 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
691 -> 692 ;
693 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
691 -> 693 ;
694 [label="entropy = 0.0\nsamples = 0.5%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
686 -> 694 ;
695 [label="X[147] <= 0.0\nentropy = 1.5\nsamples = 0.6%\nvalue = [0.0, 0.0, 0.06, 0.53, 0.0, 0.0, 0.35, 0.06, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
685 -> 695 ;
696 [label="X[290] <= -0.01\nentropy = 1.06\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.12, 0.0, 0.0, 0.0, 0.75, 0.12, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
695 -> 696 ;
697 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
696 -> 697 ;
698 [label="X[849] <= -0.02\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
696 -> 698 ;
699 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
698 -> 699 ;
700 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
698 -> 700 ;
701 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
695 -> 701 ;
702 [label="X[214] <= -0.01\nentropy = 3.18\nsamples = 4.4%\nvalue = [0.05, 0.19, 0.06, 0.02, 0.05, 0.0, 0.08, 0.19, 0.08\n0.0, 0.02, 0.0, 0.08, 0.01, 0.18]\nclass = Washington Post"] ;
662 -> 702 ;
703 [label="X[99] <= 0.02\nentropy = 2.8\nsamples = 1.8%\nvalue = [0.0, 0.04, 0.14, 0.0, 0.02, 0.0, 0.14, 0.25, 0.07\n0.0, 0.0, 0.0, 0.18, 0.02, 0.14]\nclass = Reuters"] ;
702 -> 703 ;
704 [label="X[234] <= 0.0\nentropy = 2.35\nsamples = 1.0%\nvalue = [0.0, 0.0, 0.26, 0.0, 0.03, 0.0, 0.03, 0.0, 0.13, 0.0\n0.0, 0.0, 0.32, 0.03, 0.19]\nclass = Atlantic"] ;
703 -> 704 ;
705 [label="X[147] <= -0.01\nentropy = 2.13\nsamples = 0.7%\nvalue = [0.0, 0.0, 0.38, 0.0, 0.05, 0.0, 0.05, 0.0, 0.19, 0.0\n0.0, 0.0, 0.0, 0.05, 0.29]\nclass = CNN"] ;
704 -> 705 ;
706 [label="X[366] <= 0.04\nentropy = 0.97\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0\n0.0, 0.0, 0.0, 0.0, 0.6]\nclass = Fox News"] ;
705 -> 706 ;
707 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
706 -> 707 ;
708 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
706 -> 708 ;
709 [label="X[174] <= 0.01\nentropy = 1.28\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.73, 0.0, 0.09, 0.0, 0.09, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.09, 0.0]\nclass = CNN"] ;
705 -> 709 ;
710 [label="X[31] <= -0.11\nentropy = 1.58\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.33, 0.0, 0.33, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.33, 0.0]\nclass = Buzzfeed News"] ;
709 -> 710 ;
711 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
710 -> 711 ;
712 [label="X[904] <= -0.01\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.5, 0.0]\nclass = Buzzfeed News"] ;
710 -> 712 ;
713 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
712 -> 713 ;
714 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
712 -> 714 ;
715 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
709 -> 715 ;
716 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
704 -> 716 ;
717 [label="X[915] <= -0.0\nentropy = 1.57\nsamples = 0.8%\nvalue = [0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.28, 0.56, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.08]\nclass = Reuters"] ;
703 -> 717 ;
718 [label="X[7] <= -0.08\nentropy = 1.31\nsamples = 0.4%\nvalue = [0.0, 0.18, 0.0, 0.0, 0.0, 0.0, 0.64, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.18]\nclass = New York Times"] ;
717 -> 718 ;
719 [label="X[920] <= 0.0\nentropy = 1.0\nsamples = 0.2%\nvalue = [0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.5]\nclass = Washington Post"] ;
718 -> 719 ;
720 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
719 -> 720 ;
721 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
719 -> 721 ;
722 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
718 -> 722 ;
723 [label="entropy = 0.0\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
717 -> 723 ;
724 [label="X[211] <= -0.03\nentropy = 2.77\nsamples = 2.6%\nvalue = [0.08, 0.3, 0.0, 0.04, 0.08, 0.0, 0.03, 0.14, 0.08\n0.0, 0.04, 0.0, 0.0, 0.0, 0.21]\nclass = Washington Post"] ;
702 -> 724 ;
725 [label="X[15] <= -0.02\nentropy = 2.21\nsamples = 0.6%\nvalue = [0.0, 0.06, 0.0, 0.18, 0.35, 0.0, 0.0, 0.06, 0.29\n0.0, 0.0, 0.0, 0.0, 0.0, 0.06]\nclass = Buzzfeed News"] ;
724 -> 725 ;
726 [label="X[330] <= 0.01\nentropy = 1.35\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.33, 0.0, 0.0, 0.0, 0.0, 0.56, 0.0\n0.0, 0.0, 0.0, 0.0, 0.11]\nclass = New York Post"] ;
725 -> 726 ;
727 [label="X[512] <= 0.0\nentropy = 0.81\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.25]\nclass = Business Insider"] ;
726 -> 727 ;
728 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
727 -> 728 ;
729 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
727 -> 729 ;
730 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
726 -> 730 ;
731 [label="X[263] <= -0.03\nentropy = 1.06\nsamples = 0.3%\nvalue = [0.0, 0.12, 0.0, 0.0, 0.75, 0.0, 0.0, 0.12, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
725 -> 731 ;
732 [label="X[750] <= -0.01\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
731 -> 732 ;
733 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
732 -> 733 ;
734 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
732 -> 734 ;
735 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
731 -> 735 ;
736 [label="X[82] <= -0.01\nentropy = 2.29\nsamples = 2.0%\nvalue = [0.1, 0.37, 0.0, 0.0, 0.0, 0.0, 0.03, 0.17, 0.02, 0.0\n0.05, 0.0, 0.0, 0.0, 0.25]\nclass = Washington Post"] ;
724 -> 736 ;
737 [label="X[596] <= -0.02\nentropy = 1.52\nsamples = 1.0%\nvalue = [0.0, 0.38, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.03, 0.0\n0.03, 0.0, 0.0, 0.0, 0.52]\nclass = Fox News"] ;
736 -> 737 ;
738 [label="X[445] <= 0.01\nentropy = 0.44\nsamples = 0.3%\nvalue = [0.0, 0.91, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.09, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
737 -> 738 ;
739 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
738 -> 739 ;
740 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
738 -> 740 ;
741 [label="X[406] <= -0.01\nentropy = 0.91\nsamples = 0.7%\nvalue = [0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.06, 0.0, 0.06, 0.0\n0.0, 0.0, 0.0, 0.0, 0.83]\nclass = Fox News"] ;
737 -> 741 ;
742 [label="X[202] <= 0.0\nentropy = 1.58\nsamples = 0.2%\nvalue = [0.0, 0.33, 0.0, 0.0, 0.0, 0.0, 0.33, 0.0, 0.33, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
741 -> 742 ;
743 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
742 -> 743 ;
744 [label="X[402] <= -0.04\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
742 -> 744 ;
745 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
744 -> 745 ;
746 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
744 -> 746 ;
747 [label="entropy = 0.0\nsamples = 0.6%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
741 -> 747 ;
748 [label="X[221] <= -0.0\nentropy = 1.95\nsamples = 1.0%\nvalue = [0.2, 0.37, 0.0, 0.0, 0.0, 0.0, 0.03, 0.33, 0.0, 0.0\n0.07, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
736 -> 748 ;
749 [label="X[404] <= -0.01\nentropy = 0.95\nsamples = 0.5%\nvalue = [0.0, 0.79, 0.0, 0.0, 0.0, 0.0, 0.07, 0.14, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
748 -> 749 ;
750 [label="X[164] <= 0.0\nentropy = 0.92\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33, 0.67, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
749 -> 750 ;
751 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
750 -> 751 ;
752 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
750 -> 752 ;
753 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
749 -> 753 ;
754 [label="X[84] <= -0.03\nentropy = 1.41\nsamples = 0.5%\nvalue = [0.38, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0\n0.12, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
748 -> 754 ;
755 [label="X[344] <= 0.04\nentropy = 0.81\nsamples = 0.3%\nvalue = [0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.25, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
754 -> 755 ;
756 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
755 -> 756 ;
757 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
755 -> 757 ;
758 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
754 -> 758 ;
759 [label="X[0] <= 0.33\nentropy = 3.72\nsamples = 38.6%\nvalue = [0.07, 0.08, 0.06, 0.08, 0.06, 0.05, 0.0, 0.05, 0.03\n0.1, 0.15, 0.05, 0.08, 0.1, 0.05]\nclass = Talking Points Memo"] ;
1 -> 759 ;
760 [label="X[6] <= 0.02\nentropy = 3.62\nsamples = 26.1%\nvalue = [0.1, 0.05, 0.04, 0.1, 0.07, 0.04, 0.01, 0.03, 0.04\n0.07, 0.19, 0.04, 0.05, 0.11, 0.06]\nclass = Talking Points Memo"] ;
759 -> 760 ;
761 [label="X[57] <= 0.09\nentropy = 3.52\nsamples = 16.7%\nvalue = [0.12, 0.06, 0.03, 0.13, 0.07, 0.04, 0.0, 0.03, 0.05\n0.05, 0.23, 0.04, 0.04, 0.04, 0.05]\nclass = Talking Points Memo"] ;
760 -> 761 ;
762 [label="X[5] <= 0.03\nentropy = 3.48\nsamples = 15.7%\nvalue = [0.1, 0.07, 0.04, 0.14, 0.08, 0.05, 0.0, 0.03, 0.06\n0.05, 0.24, 0.04, 0.04, 0.03, 0.02]\nclass = Talking Points Memo"] ;
761 -> 762 ;
763 [label="X[16] <= 0.06\nentropy = 3.25\nsamples = 10.9%\nvalue = [0.11, 0.06, 0.04, 0.1, 0.07, 0.04, 0.0, 0.01, 0.07\n0.07, 0.32, 0.04, 0.02, 0.03, 0.02]\nclass = Talking Points Memo"] ;
762 -> 763 ;
764 [label="X[517] <= 0.03\nentropy = 3.08\nsamples = 9.5%\nvalue = [0.11, 0.07, 0.04, 0.12, 0.08, 0.03, 0.0, 0.01, 0.08\n0.03, 0.36, 0.04, 0.02, 0.01, 0.02]\nclass = Talking Points Memo"] ;
763 -> 764 ;
765 [label="X[668] <= 0.0\nentropy = 2.89\nsamples = 8.1%\nvalue = [0.11, 0.07, 0.01, 0.13, 0.09, 0.03, 0.0, 0.01, 0.03\n0.04, 0.4, 0.02, 0.02, 0.01, 0.02]\nclass = Talking Points Memo"] ;
764 -> 765 ;
766 [label="X[725] <= -0.01\nentropy = 2.23\nsamples = 4.7%\nvalue = [0.11, 0.09, 0.01, 0.09, 0.09, 0.0, 0.0, 0.0, 0.0\n0.04, 0.53, 0.0, 0.0, 0.01, 0.04]\nclass = Talking Points Memo"] ;
765 -> 766 ;
767 [label="X[757] <= -0.01\nentropy = 1.82\nsamples = 1.7%\nvalue = [0.2, 0.04, 0.02, 0.0, 0.22, 0.0, 0.0, 0.0, 0.0, 0.0\n0.51, 0.0, 0.0, 0.02, 0.0]\nclass = Talking Points Memo"] ;
766 -> 767 ;
768 [label="X[8] <= -0.03\nentropy = 0.98\nsamples = 0.5%\nvalue = [0.42, 0.0, 0.0, 0.0, 0.58, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
767 -> 768 ;
769 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
768 -> 769 ;
770 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
768 -> 770 ;
771 [label="X[11] <= -0.03\nentropy = 1.24\nsamples = 1.3%\nvalue = [0.08, 0.06, 0.03, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0\n0.78, 0.0, 0.0, 0.03, 0.0]\nclass = Talking Points Memo"] ;
767 -> 771 ;
772 [label="X[972] <= -0.01\nentropy = 1.79\nsamples = 0.3%\nvalue = [0.5, 0.17, 0.17, 0.0, 0.17, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
771 -> 772 ;
773 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
772 -> 773 ;
774 [label="X[469] <= -0.0\nentropy = 1.58\nsamples = 0.2%\nvalue = [0.0, 0.33, 0.33, 0.0, 0.33, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
772 -> 774 ;
775 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
774 -> 775 ;
776 [label="X[807] <= -0.03\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.5, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
774 -> 776 ;
777 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
776 -> 777 ;
778 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
776 -> 778 ;
779 [label="X[853] <= -0.01\nentropy = 0.42\nsamples = 1.0%\nvalue = [0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.93, 0.0, 0.0, 0.03, 0.0]\nclass = Talking Points Memo"] ;
771 -> 779 ;
780 [label="X[442] <= -0.0\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.5, 0.0]\nclass = Washington Post"] ;
779 -> 780 ;
781 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
780 -> 781 ;
782 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
780 -> 782 ;
783 [label="entropy = 0.0\nsamples = 0.9%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
779 -> 783 ;
784 [label="X[972] <= 0.0\nentropy = 2.0\nsamples = 2.9%\nvalue = [0.06, 0.13, 0.0, 0.15, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06\n0.54, 0.0, 0.0, 0.0, 0.06]\nclass = Talking Points Memo"] ;
766 -> 784 ;
785 [label="X[581] <= 0.04\nentropy = 1.06\nsamples = 1.5%\nvalue = [0.0, 0.12, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.78, 0.0, 0.0, 0.0, 0.06]\nclass = Talking Points Memo"] ;
784 -> 785 ;
786 [label="X[66] <= -0.01\nentropy = 0.61\nsamples = 1.4%\nvalue = [0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.89, 0.0, 0.0, 0.0, 0.07]\nclass = Talking Points Memo"] ;
785 -> 786 ;
787 [label="X[454] <= -0.01\nentropy = 0.81\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.75]\nclass = Fox News"] ;
786 -> 787 ;
788 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
787 -> 788 ;
789 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
787 -> 789 ;
790 [label="X[677] <= -0.04\nentropy = 0.17\nsamples = 1.2%\nvalue = [0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.98, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
786 -> 790 ;
791 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
790 -> 791 ;
792 [label="entropy = 0.0\nsamples = 1.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
790 -> 792 ;
793 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
785 -> 793 ;
794 [label="X[90] <= -0.0\nentropy = 2.43\nsamples = 1.4%\nvalue = [0.15, 0.15, 0.0, 0.32, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15\n0.18, 0.0, 0.0, 0.0, 0.06]\nclass = Business Insider"] ;
784 -> 794 ;
795 [label="X[35] <= 0.01\nentropy = 1.65\nsamples = 0.5%\nvalue = [0.42, 0.42, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.08, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
794 -> 795 ;
796 [label="X[890] <= 0.0\nentropy = 0.65\nsamples = 0.3%\nvalue = [0.0, 0.83, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.17, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
795 -> 796 ;
797 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
796 -> 797 ;
798 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
796 -> 798 ;
799 [label="X[161] <= -0.01\nentropy = 0.65\nsamples = 0.3%\nvalue = [0.83, 0.0, 0.0, 0.17, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
795 -> 799 ;
800 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
799 -> 800 ;
801 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
799 -> 801 ;
802 [label="X[226] <= -0.02\nentropy = 1.8\nsamples = 0.9%\nvalue = [0.0, 0.0, 0.0, 0.45, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23\n0.23, 0.0, 0.0, 0.0, 0.09]\nclass = Business Insider"] ;
794 -> 802 ;
803 [label="X[186] <= -0.04\nentropy = 0.86\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.71\n0.0, 0.0, 0.0, 0.0, 0.29]\nclass = Vox"] ;
802 -> 803 ;
804 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
803 -> 804 ;
805 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
803 -> 805 ;
806 [label="X[223] <= 0.01\nentropy = 0.92\nsamples = 0.6%\nvalue = [0.0, 0.0, 0.0, 0.67, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.33, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
802 -> 806 ;
807 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
806 -> 807 ;
808 [label="entropy = 0.0\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
806 -> 808 ;
809 [label="X[526] <= -0.01\nentropy = 3.3\nsamples = 3.5%\nvalue = [0.12, 0.03, 0.02, 0.19, 0.08, 0.08, 0.0, 0.02, 0.06\n0.04, 0.22, 0.05, 0.05, 0.02, 0.0]\nclass = Talking Points Memo"] ;
765 -> 809 ;
810 [label="X[95] <= 0.0\nentropy = 1.61\nsamples = 0.7%\nvalue = [0.0, 0.0, 0.0, 0.62, 0.04, 0.17, 0.0, 0.0, 0.0, 0.04\n0.0, 0.12, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
809 -> 810 ;
811 [label="X[603] <= -0.04\nentropy = 0.34\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.94, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
810 -> 811 ;
812 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
811 -> 812 ;
813 [label="entropy = 0.0\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
811 -> 813 ;
814 [label="X[395] <= 0.01\nentropy = 1.41\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.12\n0.0, 0.38, 0.0, 0.0, 0.0]\nclass = NPR"] ;
810 -> 814 ;
815 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
814 -> 815 ;
816 [label="X[41] <= -0.0\nentropy = 0.81\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25\n0.0, 0.75, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
814 -> 816 ;
817 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
816 -> 817 ;
818 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
816 -> 818 ;
819 [label="X[316] <= 0.01\nentropy = 3.23\nsamples = 2.8%\nvalue = [0.15, 0.04, 0.03, 0.04, 0.1, 0.06, 0.0, 0.03, 0.08\n0.04, 0.3, 0.03, 0.07, 0.03, 0.0]\nclass = Talking Points Memo"] ;
809 -> 819 ;
820 [label="X[770] <= 0.01\nentropy = 2.57\nsamples = 2.1%\nvalue = [0.21, 0.0, 0.0, 0.06, 0.08, 0.06, 0.0, 0.0, 0.02\n0.06, 0.4, 0.0, 0.08, 0.04, 0.0]\nclass = Talking Points Memo"] ;
819 -> 820 ;
821 [label="X[45] <= -0.0\nentropy = 2.06\nsamples = 1.4%\nvalue = [0.08, 0.0, 0.0, 0.08, 0.11, 0.08, 0.0, 0.0, 0.0, 0.0\n0.54, 0.0, 0.11, 0.0, 0.0]\nclass = Talking Points Memo"] ;
820 -> 821 ;
822 [label="X[432] <= 0.02\nentropy = 1.16\nsamples = 0.8%\nvalue = [0.12, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.71, 0.0, 0.17, 0.0, 0.0]\nclass = Talking Points Memo"] ;
821 -> 822 ;
823 [label="X[766] <= 0.04\nentropy = 0.31\nsamples = 0.6%\nvalue = [0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.94, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
822 -> 823 ;
824 [label="entropy = 0.0\nsamples = 0.6%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
823 -> 824 ;
825 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
823 -> 825 ;
826 [label="X[908] <= -0.02\nentropy = 0.92\nsamples = 0.2%\nvalue = [0.33, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.67, 0.0, 0.0]\nclass = Atlantic"] ;
822 -> 826 ;
827 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
826 -> 827 ;
828 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
826 -> 828 ;
829 [label="X[947] <= -0.01\nentropy = 1.99\nsamples = 0.6%\nvalue = [0.0, 0.0, 0.0, 0.23, 0.31, 0.23, 0.0, 0.0, 0.0, 0.0\n0.23, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
821 -> 829 ;
830 [label="X[895] <= 0.02\nentropy = 1.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0\n0.5, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
829 -> 830 ;
831 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
830 -> 831 ;
832 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
830 -> 832 ;
833 [label="X[569] <= 0.01\nentropy = 0.99\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.43, 0.57, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
829 -> 833 ;
834 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
833 -> 834 ;
835 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
833 -> 835 ;
836 [label="X[655] <= 0.0\nentropy = 1.86\nsamples = 0.7%\nvalue = [0.53, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07, 0.2\n0.07, 0.0, 0.0, 0.13, 0.0]\nclass = Breitbart"] ;
820 -> 836 ;
837 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
836 -> 837 ;
838 [label="X[717] <= 0.0\nentropy = 1.84\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14, 0.43\n0.14, 0.0, 0.0, 0.29, 0.0]\nclass = Vox"] ;
836 -> 838 ;
839 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
838 -> 839 ;
840 [label="X[464] <= 0.01\nentropy = 1.5\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0\n0.25, 0.0, 0.0, 0.5, 0.0]\nclass = National Review"] ;
838 -> 840 ;
841 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
840 -> 841 ;
842 [label="X[428] <= -0.03\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0\n0.5, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
840 -> 842 ;
843 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
842 -> 843 ;
844 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
842 -> 844 ;
845 [label="X[23] <= 0.02\nentropy = 2.82\nsamples = 0.7%\nvalue = [0.0, 0.16, 0.11, 0.0, 0.16, 0.05, 0.0, 0.11, 0.26\n0.0, 0.0, 0.11, 0.05, 0.0, 0.0]\nclass = New York Post"] ;
819 -> 845 ;
846 [label="X[831] <= 0.01\nentropy = 1.89\nsamples = 0.3%\nvalue = [0.0, 0.33, 0.0, 0.0, 0.33, 0.11, 0.0, 0.0, 0.0, 0.0\n0.0, 0.22, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
845 -> 846 ;
847 [label="X[25] <= 0.0\nentropy = 0.97\nsamples = 0.2%\nvalue = [0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.4, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
846 -> 847 ;
848 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
847 -> 848 ;
849 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
847 -> 849 ;
850 [label="X[525] <= -0.02\nentropy = 0.81\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.75, 0.25, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
846 -> 850 ;
851 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
850 -> 851 ;
852 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
850 -> 852 ;
853 [label="X[883] <= -0.0\nentropy = 1.76\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.5, 0.0\n0.0, 0.0, 0.1, 0.0, 0.0]\nclass = New York Post"] ;
845 -> 853 ;
854 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
853 -> 854 ;
855 [label="X[577] <= -0.01\nentropy = 1.52\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0\n0.0, 0.0, 0.2, 0.0, 0.0]\nclass = CNN"] ;
853 -> 855 ;
856 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
855 -> 856 ;
857 [label="X[78] <= -0.0\nentropy = 0.92\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.67, 0.0, 0.0\n0.0, 0.0, 0.33, 0.0, 0.0]\nclass = Reuters"] ;
855 -> 857 ;
858 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
857 -> 858 ;
859 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
857 -> 859 ;
860 [label="X[881] <= -0.01\nentropy = 2.68\nsamples = 1.4%\nvalue = [0.06, 0.06, 0.15, 0.04, 0.06, 0.0, 0.0, 0.0, 0.34\n0.0, 0.13, 0.15, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
764 -> 860 ;
861 [label="X[648] <= -0.0\nentropy = 1.38\nsamples = 0.7%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.59, 0.0\n0.22, 0.19, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
860 -> 861 ;
862 [label="entropy = 0.0\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
861 -> 862 ;
863 [label="X[27] <= -0.01\nentropy = 0.99\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.55, 0.45, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
861 -> 863 ;
864 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
863 -> 864 ;
865 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
863 -> 865 ;
866 [label="X[927] <= -0.0\nentropy = 2.43\nsamples = 0.6%\nvalue = [0.15, 0.15, 0.35, 0.1, 0.15, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.1, 0.0, 0.0, 0.0]\nclass = CNN"] ;
860 -> 866 ;
867 [label="X[190] <= -0.03\nentropy = 1.97\nsamples = 0.3%\nvalue = [0.3, 0.0, 0.0, 0.2, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.2, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
866 -> 867 ;
868 [label="X[405] <= -0.03\nentropy = 0.97\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.4, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
867 -> 868 ;
869 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
868 -> 869 ;
870 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
868 -> 870 ;
871 [label="X[394] <= -0.01\nentropy = 0.97\nsamples = 0.2%\nvalue = [0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.4, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
867 -> 871 ;
872 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
871 -> 872 ;
873 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
871 -> 873 ;
874 [label="X[338] <= 0.02\nentropy = 0.88\nsamples = 0.3%\nvalue = [0.0, 0.3, 0.7, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
866 -> 874 ;
875 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
874 -> 875 ;
876 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
874 -> 876 ;
877 [label="X[409] <= 0.01\nentropy = 2.79\nsamples = 1.4%\nvalue = [0.12, 0.05, 0.07, 0.0, 0.02, 0.09, 0.0, 0.0, 0.0\n0.33, 0.07, 0.0, 0.07, 0.19, 0.0]\nclass = Vox"] ;
763 -> 877 ;
878 [label="X[291] <= -0.0\nentropy = 1.88\nsamples = 0.9%\nvalue = [0.19, 0.0, 0.0, 0.0, 0.04, 0.15, 0.0, 0.0, 0.0, 0.52\n0.11, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
877 -> 878 ;
879 [label="X[76] <= 0.0\nentropy = 1.78\nsamples = 0.4%\nvalue = [0.42, 0.0, 0.0, 0.0, 0.08, 0.33, 0.0, 0.0, 0.0, 0.0\n0.17, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
878 -> 879 ;
880 [label="X[160] <= 0.04\nentropy = 0.65\nsamples = 0.2%\nvalue = [0.83, 0.0, 0.0, 0.0, 0.17, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
879 -> 880 ;
881 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
880 -> 881 ;
882 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
880 -> 882 ;
883 [label="X[941] <= -0.0\nentropy = 0.92\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.67, 0.0, 0.0, 0.0, 0.0\n0.33, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
879 -> 883 ;
884 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
883 -> 884 ;
885 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
883 -> 885 ;
886 [label="X[872] <= 0.01\nentropy = 0.35\nsamples = 0.5%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.93\n0.07, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
878 -> 886 ;
887 [label="entropy = 0.0\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
886 -> 887 ;
888 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
886 -> 888 ;
889 [label="X[726] <= -0.01\nentropy = 1.78\nsamples = 0.5%\nvalue = [0.0, 0.12, 0.19, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.19, 0.5, 0.0]\nclass = National Review"] ;
877 -> 889 ;
890 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
889 -> 890 ;
891 [label="X[210] <= 0.02\nentropy = 1.56\nsamples = 0.3%\nvalue = [0.0, 0.25, 0.38, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.38, 0.0, 0.0]\nclass = CNN"] ;
889 -> 891 ;
892 [label="X[646] <= -0.0\nentropy = 0.97\nsamples = 0.2%\nvalue = [0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.6, 0.0, 0.0]\nclass = Atlantic"] ;
891 -> 892 ;
893 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
892 -> 893 ;
894 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
892 -> 894 ;
895 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
891 -> 895 ;
896 [label="X[132] <= -0.01\nentropy = 3.48\nsamples = 4.8%\nvalue = [0.09, 0.07, 0.03, 0.25, 0.09, 0.07, 0.01, 0.1, 0.04\n0.0, 0.06, 0.04, 0.07, 0.04, 0.04]\nclass = Business Insider"] ;
762 -> 896 ;
897 [label="X[547] <= -0.01\nentropy = 2.4\nsamples = 1.5%\nvalue = [0.11, 0.02, 0.0, 0.49, 0.0, 0.05, 0.04, 0.11, 0.0\n0.0, 0.07, 0.09, 0.0, 0.02, 0.0]\nclass = Business Insider"] ;
896 -> 897 ;
898 [label="X[353] <= -0.01\nentropy = 2.4\nsamples = 0.7%\nvalue = [0.24, 0.0, 0.0, 0.0, 0.0, 0.14, 0.1, 0.29, 0.0, 0.0\n0.19, 0.05, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
897 -> 898 ;
899 [label="X[156] <= -0.01\nentropy = 1.85\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.2, 0.0, 0.0, 0.0\n0.4, 0.1, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
898 -> 899 ;
900 [label="X[958] <= -0.01\nentropy = 0.97\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.4, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
899 -> 900 ;
901 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
900 -> 901 ;
902 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
900 -> 902 ;
903 [label="X[193] <= -0.03\nentropy = 0.72\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.8, 0.2, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
899 -> 903 ;
904 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
903 -> 904 ;
905 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
903 -> 905 ;
906 [label="X[550] <= 0.0\nentropy = 0.99\nsamples = 0.4%\nvalue = [0.45, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.55, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
898 -> 906 ;
907 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
906 -> 907 ;
908 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
906 -> 908 ;
909 [label="X[754] <= 0.0\nentropy = 1.08\nsamples = 0.8%\nvalue = [0.03, 0.03, 0.0, 0.79, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.12, 0.0, 0.03, 0.0]\nclass = Business Insider"] ;
897 -> 909 ;
910 [label="X[917] <= 0.01\nentropy = 1.66\nsamples = 0.3%\nvalue = [0.14, 0.14, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.57, 0.0, 0.14, 0.0]\nclass = Guardian"] ;
909 -> 910 ;
911 [label="X[646] <= 0.0\nentropy = 1.58\nsamples = 0.2%\nvalue = [0.33, 0.33, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.33, 0.0]\nclass = Breitbart"] ;
910 -> 911 ;
912 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
911 -> 912 ;
913 [label="X[394] <= 0.01\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.5, 0.0]\nclass = Washington Post"] ;
911 -> 913 ;
914 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
913 -> 914 ;
915 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
913 -> 915 ;
916 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
910 -> 916 ;
917 [label="entropy = 0.0\nsamples = 0.6%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
909 -> 917 ;
918 [label="X[59] <= 0.01\nentropy = 3.54\nsamples = 3.3%\nvalue = [0.08, 0.11, 0.05, 0.08, 0.14, 0.08, 0.0, 0.1, 0.06\n0.0, 0.05, 0.01, 0.12, 0.05, 0.06]\nclass = Buzzfeed News"] ;
896 -> 918 ;
919 [label="X[969] <= -0.0\nentropy = 3.28\nsamples = 2.1%\nvalue = [0.02, 0.17, 0.08, 0.08, 0.04, 0.06, 0.0, 0.15, 0.06\n0.0, 0.0, 0.02, 0.19, 0.08, 0.06]\nclass = Atlantic"] ;
918 -> 919 ;
920 [label="X[989] <= -0.01\nentropy = 1.94\nsamples = 0.8%\nvalue = [0.05, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.27, 0.0, 0.0\n0.0, 0.0, 0.45, 0.14, 0.0]\nclass = Atlantic"] ;
919 -> 920 ;
921 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
920 -> 921 ;
922 [label="X[726] <= 0.0\nentropy = 1.73\nsamples = 0.5%\nvalue = [0.08, 0.0, 0.17, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0\n0.0, 0.0, 0.0, 0.25, 0.0]\nclass = Reuters"] ;
920 -> 922 ;
923 [label="X[806] <= 0.01\nentropy = 1.46\nsamples = 0.3%\nvalue = [0.17, 0.0, 0.33, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.5, 0.0]\nclass = National Review"] ;
922 -> 923 ;
924 [label="X[357] <= -0.02\nentropy = 0.92\nsamples = 0.1%\nvalue = [0.33, 0.0, 0.67, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
923 -> 924 ;
925 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
924 -> 925 ;
926 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
924 -> 926 ;
927 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
923 -> 927 ;
928 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
922 -> 928 ;
929 [label="X[227] <= 0.02\nentropy = 3.01\nsamples = 1.3%\nvalue = [0.0, 0.3, 0.07, 0.13, 0.07, 0.1, 0.0, 0.07, 0.1, 0.0\n0.0, 0.03, 0.0, 0.03, 0.1]\nclass = Washington Post"] ;
919 -> 929 ;
930 [label="X[178] <= -0.02\nentropy = 2.28\nsamples = 0.9%\nvalue = [0.0, 0.41, 0.09, 0.18, 0.0, 0.14, 0.0, 0.0, 0.14\n0.0, 0.0, 0.05, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
929 -> 930 ;
931 [label="X[877] <= 0.01\nentropy = 1.46\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.33, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0\n0.0, 0.17, 0.0, 0.0, 0.0]\nclass = NPR"] ;
930 -> 931 ;
932 [label="X[61] <= -0.06\nentropy = 0.92\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.67, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.33, 0.0, 0.0, 0.0]\nclass = CNN"] ;
931 -> 932 ;
933 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
932 -> 933 ;
934 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
932 -> 934 ;
935 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
931 -> 935 ;
936 [label="X[569] <= 0.01\nentropy = 1.42\nsamples = 0.7%\nvalue = [0.0, 0.56, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.19, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
930 -> 936 ;
937 [label="X[705] <= -0.01\nentropy = 0.81\nsamples = 0.5%\nvalue = [0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
936 -> 937 ;
938 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
937 -> 938 ;
939 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
937 -> 939 ;
940 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
936 -> 940 ;
941 [label="X[580] <= 0.0\nentropy = 1.91\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.25, 0.0, 0.0\n0.0, 0.0, 0.0, 0.12, 0.38]\nclass = Fox News"] ;
929 -> 941 ;
942 [label="X[333] <= 0.02\nentropy = 0.81\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.25, 0.75]\nclass = Fox News"] ;
941 -> 942 ;
943 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
942 -> 943 ;
944 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
942 -> 944 ;
945 [label="X[13] <= 0.01\nentropy = 1.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.5, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
941 -> 945 ;
946 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
945 -> 946 ;
947 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
945 -> 947 ;
948 [label="X[341] <= 0.01\nentropy = 2.58\nsamples = 1.2%\nvalue = [0.19, 0.0, 0.0, 0.1, 0.32, 0.13, 0.0, 0.0, 0.06, 0.0\n0.13, 0.0, 0.0, 0.0, 0.06]\nclass = Buzzfeed News"] ;
918 -> 948 ;
949 [label="X[182] <= -0.01\nentropy = 1.79\nsamples = 0.9%\nvalue = [0.27, 0.0, 0.0, 0.0, 0.45, 0.0, 0.0, 0.0, 0.0, 0.0\n0.18, 0.0, 0.0, 0.0, 0.09]\nclass = Buzzfeed News"] ;
948 -> 949 ;
950 [label="X[642] <= 0.01\nentropy = 1.49\nsamples = 0.5%\nvalue = [0.45, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.36, 0.0, 0.0, 0.0, 0.18]\nclass = Breitbart"] ;
949 -> 950 ;
951 [label="X[520] <= -0.02\nentropy = 0.92\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.67, 0.0, 0.0, 0.0, 0.33]\nclass = Talking Points Memo"] ;
950 -> 951 ;
952 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
951 -> 952 ;
953 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
951 -> 953 ;
954 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
950 -> 954 ;
955 [label="X[502] <= -0.02\nentropy = 0.44\nsamples = 0.4%\nvalue = [0.09, 0.0, 0.0, 0.0, 0.91, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
949 -> 955 ;
956 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
955 -> 956 ;
957 [label="entropy = 0.0\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
955 -> 957 ;
958 [label="X[796] <= -0.01\nentropy = 1.53\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.33, 0.0, 0.44, 0.0, 0.0, 0.22, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
948 -> 958 ;
959 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
958 -> 959 ;
960 [label="X[245] <= -0.03\nentropy = 0.97\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
958 -> 960 ;
961 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
960 -> 961 ;
962 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
960 -> 962 ;
963 [label="X[72] <= -0.03\nentropy = 1.58\nsamples = 1.0%\nvalue = [0.35, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.06, 0.0, 0.0, 0.09, 0.5]\nclass = Fox News"] ;
761 -> 963 ;
964 [label="entropy = 0.0\nsamples = 0.6%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
963 -> 964 ;
965 [label="X[384] <= 0.02\nentropy = 1.16\nsamples = 0.5%\nvalue = [0.71, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.12, 0.0, 0.0, 0.18, 0.0]\nclass = Breitbart"] ;
963 -> 965 ;
966 [label="entropy = 0.0\nsamples = 0.4%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
965 -> 966 ;
967 [label="X[879] <= 0.0\nentropy = 0.97\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.4, 0.0, 0.0, 0.6, 0.0]\nclass = National Review"] ;
965 -> 967 ;
968 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
967 -> 968 ;
969 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
967 -> 969 ;
970 [label="X[3] <= -0.04\nentropy = 3.48\nsamples = 9.4%\nvalue = [0.06, 0.03, 0.04, 0.04, 0.07, 0.02, 0.01, 0.03, 0.02\n0.12, 0.12, 0.04, 0.07, 0.24, 0.08]\nclass = National Review"] ;
760 -> 970 ;
971 [label="X[500] <= -0.01\nentropy = 2.74\nsamples = 3.4%\nvalue = [0.04, 0.07, 0.0, 0.01, 0.04, 0.02, 0.0, 0.0, 0.0\n0.18, 0.04, 0.02, 0.19, 0.35, 0.05]\nclass = National Review"] ;
970 -> 971 ;
972 [label="X[55] <= -0.01\nentropy = 2.13\nsamples = 0.7%\nvalue = [0.12, 0.28, 0.0, 0.0, 0.16, 0.0, 0.0, 0.0, 0.0, 0.0\n0.08, 0.0, 0.0, 0.36, 0.0]\nclass = National Review"] ;
971 -> 972 ;
973 [label="X[158] <= -0.02\nentropy = 0.81\nsamples = 0.4%\nvalue = [0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.75, 0.0]\nclass = National Review"] ;
972 -> 973 ;
974 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
973 -> 974 ;
975 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
973 -> 975 ;
976 [label="X[155] <= -0.0\nentropy = 1.42\nsamples = 0.3%\nvalue = [0.0, 0.54, 0.0, 0.0, 0.31, 0.0, 0.0, 0.0, 0.0, 0.0\n0.15, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
972 -> 976 ;
977 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
976 -> 977 ;
978 [label="X[610] <= -0.02\nentropy = 0.92\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.67, 0.0, 0.0, 0.0, 0.0, 0.0\n0.33, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
976 -> 978 ;
979 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
978 -> 979 ;
980 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
978 -> 980 ;
981 [label="X[18] <= -0.04\nentropy = 2.35\nsamples = 2.7%\nvalue = [0.01, 0.0, 0.0, 0.01, 0.0, 0.03, 0.0, 0.0, 0.0, 0.23\n0.03, 0.03, 0.25, 0.35, 0.06]\nclass = National Review"] ;
971 -> 981 ;
982 [label="X[113] <= 0.01\nentropy = 1.67\nsamples = 0.7%\nvalue = [0.05, 0.0, 0.0, 0.05, 0.0, 0.05, 0.0, 0.0, 0.0, 0.64\n0.0, 0.0, 0.05, 0.0, 0.18]\nclass = Vox"] ;
981 -> 982 ;
983 [label="entropy = 0.0\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
982 -> 983 ;
984 [label="X[40] <= 0.06\nentropy = 2.0\nsamples = 0.3%\nvalue = [0.12, 0.0, 0.0, 0.12, 0.0, 0.12, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.12, 0.0, 0.5]\nclass = Fox News"] ;
982 -> 984 ;
985 [label="X[818] <= 0.0\nentropy = 2.0\nsamples = 0.2%\nvalue = [0.25, 0.0, 0.0, 0.25, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.25, 0.0, 0.0]\nclass = Breitbart"] ;
984 -> 985 ;
986 [label="X[638] <= -0.02\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.5, 0.0, 0.0]\nclass = Breitbart"] ;
985 -> 986 ;
987 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
986 -> 987 ;
988 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
986 -> 988 ;
989 [label="X[281] <= -0.04\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.5, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
985 -> 989 ;
990 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
989 -> 990 ;
991 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
989 -> 991 ;
992 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
984 -> 992 ;
993 [label="X[248] <= -0.01\nentropy = 1.86\nsamples = 2.0%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.07\n0.04, 0.04, 0.33, 0.49, 0.02]\nclass = National Review"] ;
981 -> 993 ;
994 [label="X[968] <= 0.01\nentropy = 1.32\nsamples = 0.8%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0\n0.04, 0.04, 0.72, 0.16, 0.0]\nclass = Atlantic"] ;
993 -> 994 ;
995 [label="X[43] <= -0.05\nentropy = 1.25\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.17, 0.0, 0.0, 0.0, 0.0\n0.0, 0.17, 0.0, 0.67, 0.0]\nclass = National Review"] ;
994 -> 995 ;
996 [label="X[734] <= -0.01\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0\n0.0, 0.5, 0.0, 0.0, 0.0]\nclass = NPR"] ;
995 -> 996 ;
997 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
996 -> 997 ;
998 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
996 -> 998 ;
999 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
995 -> 999 ;
1000 [label="X[159] <= -0.05\nentropy = 0.3\nsamples = 0.5%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.05, 0.0, 0.95, 0.0, 0.0]\nclass = Atlantic"] ;
994 -> 1000 ;
1001 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
1000 -> 1001 ;
1002 [label="entropy = 0.0\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
1000 -> 1002 ;
1003 [label="X[368] <= 0.03\nentropy = 1.17\nsamples = 1.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13\n0.03, 0.03, 0.0, 0.77, 0.03]\nclass = National Review"] ;
993 -> 1003 ;
1004 [label="X[77] <= 0.03\nentropy = 0.7\nsamples = 1.0%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.04, 0.04, 0.0, 0.88, 0.04]\nclass = National Review"] ;
1003 -> 1004 ;
1005 [label="entropy = 0.0\nsamples = 0.8%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
1004 -> 1005 ;
1006 [label="X[866] <= -0.01\nentropy = 1.58\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.33, 0.33, 0.0, 0.0, 0.33]\nclass = Talking Points Memo"] ;
1004 -> 1006 ;
1007 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
1006 -> 1007 ;
1008 [label="X[794] <= -0.0\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.5, 0.0, 0.0, 0.5]\nclass = Guardian"] ;
1006 -> 1008 ;
1009 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
1008 -> 1009 ;
1010 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
1008 -> 1010 ;
1011 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
1003 -> 1011 ;
1012 [label="X[968] <= 0.01\nentropy = 3.48\nsamples = 6.0%\nvalue = [0.07, 0.02, 0.06, 0.06, 0.09, 0.02, 0.01, 0.05, 0.03\n0.09, 0.16, 0.06, 0.0, 0.18, 0.1]\nclass = National Review"] ;
970 -> 1012 ;
1013 [label="X[32] <= 0.01\nentropy = 2.98\nsamples = 3.7%\nvalue = [0.1, 0.01, 0.04, 0.09, 0.09, 0.01, 0.02, 0.0, 0.01\n0.11, 0.23, 0.0, 0.0, 0.24, 0.05]\nclass = National Review"] ;
1012 -> 1013 ;
1014 [label="X[250] <= -0.0\nentropy = 2.8\nsamples = 2.5%\nvalue = [0.17, 0.01, 0.06, 0.04, 0.14, 0.0, 0.03, 0.0, 0.0\n0.01, 0.32, 0.0, 0.0, 0.12, 0.09]\nclass = Talking Points Memo"] ;
1013 -> 1014 ;
1015 [label="X[800] <= -0.01\nentropy = 1.77\nsamples = 1.0%\nvalue = [0.03, 0.0, 0.1, 0.1, 0.0, 0.0, 0.07, 0.0, 0.0, 0.0\n0.63, 0.0, 0.0, 0.0, 0.07]\nclass = Talking Points Memo"] ;
1014 -> 1015 ;
1016 [label="X[713] <= 0.0\nentropy = 1.46\nsamples = 0.2%\nvalue = [0.17, 0.0, 0.0, 0.5, 0.0, 0.0, 0.33, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
1015 -> 1016 ;
1017 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
1016 -> 1017 ;
1018 [label="X[369] <= 0.02\nentropy = 0.92\nsamples = 0.1%\nvalue = [0.33, 0.0, 0.0, 0.0, 0.0, 0.0, 0.67, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
1016 -> 1018 ;
1019 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
1018 -> 1019 ;
1020 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
1018 -> 1020 ;
1021 [label="X[674] <= -0.01\nentropy = 0.94\nsamples = 0.8%\nvalue = [0.0, 0.0, 0.12, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.79, 0.0, 0.0, 0.0, 0.08]\nclass = Talking Points Memo"] ;
1015 -> 1021 ;
1022 [label="X[475] <= -0.01\nentropy = 0.97\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.4]\nclass = CNN"] ;
1021 -> 1022 ;
1023 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
1022 -> 1023 ;
1024 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
1022 -> 1024 ;
1025 [label="entropy = 0.0\nsamples = 0.6%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
1021 -> 1025 ;
1026 [label="X[61] <= 0.01\nentropy = 2.52\nsamples = 1.5%\nvalue = [0.28, 0.03, 0.03, 0.0, 0.26, 0.0, 0.0, 0.0, 0.0\n0.03, 0.08, 0.0, 0.0, 0.21, 0.1]\nclass = Breitbart"] ;
1014 -> 1026 ;
1027 [label="X[789] <= 0.0\nentropy = 2.09\nsamples = 1.0%\nvalue = [0.41, 0.04, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.07, 0.0, 0.0, 0.3, 0.15]\nclass = Breitbart"] ;
1026 -> 1027 ;
1028 [label="X[689] <= 0.01\nentropy = 1.93\nsamples = 0.6%\nvalue = [0.0, 0.07, 0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.13, 0.0, 0.0, 0.47, 0.27]\nclass = National Review"] ;
1027 -> 1028 ;
1029 [label="X[723] <= 0.01\nentropy = 1.75\nsamples = 0.4%\nvalue = [0.0, 0.12, 0.12, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.25, 0.0, 0.0, 0.0, 0.5]\nclass = Fox News"] ;
1028 -> 1029 ;
1030 [label="X[511] <= -0.02\nentropy = 1.5\nsamples = 0.2%\nvalue = [0.0, 0.25, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.5, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
1029 -> 1030 ;
1031 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
1030 -> 1031 ;
1032 [label="X[607] <= -0.03\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
1030 -> 1032 ;
1033 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
1032 -> 1033 ;
1034 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
1032 -> 1034 ;
1035 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
1029 -> 1035 ;
1036 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
1028 -> 1036 ;
1037 [label="X[360] <= -0.04\nentropy = 0.41\nsamples = 0.4%\nvalue = [0.92, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.08, 0.0]\nclass = Breitbart"] ;
1027 -> 1037 ;
1038 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
1037 -> 1038 ;
1039 [label="entropy = 0.0\nsamples = 0.4%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
1037 -> 1039 ;
1040 [label="X[221] <= 0.0\nentropy = 0.82\nsamples = 0.5%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.83, 0.0, 0.0, 0.0, 0.0, 0.08\n0.08, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
1026 -> 1040 ;
1041 [label="X[598] <= 0.03\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5\n0.5, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
1040 -> 1041 ;
1042 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
1041 -> 1042 ;
1043 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
1041 -> 1043 ;
1044 [label="entropy = 0.0\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
1040 -> 1044 ;
1045 [label="X[756] <= 0.01\nentropy = 2.14\nsamples = 1.2%\nvalue = [0.0, 0.0, 0.02, 0.15, 0.0, 0.02, 0.0, 0.0, 0.02\n0.26, 0.11, 0.0, 0.0, 0.43, 0.0]\nclass = National Review"] ;
1013 -> 1045 ;
1046 [label="X[527] <= 0.01\nentropy = 1.59\nsamples = 0.8%\nvalue = [0.0, 0.0, 0.0, 0.21, 0.0, 0.03, 0.0, 0.0, 0.03, 0.0\n0.12, 0.0, 0.0, 0.61, 0.0]\nclass = National Review"] ;
1045 -> 1046 ;
1047 [label="X[558] <= 0.0\nentropy = 0.28\nsamples = 0.5%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.95, 0.0]\nclass = National Review"] ;
1046 -> 1047 ;
1048 [label="entropy = 0.0\nsamples = 0.5%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
1047 -> 1048 ;
1049 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
1047 -> 1049 ;
1050 [label="X[249] <= -0.03\nentropy = 1.28\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.58, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0\n0.33, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
1046 -> 1050 ;
1051 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
1050 -> 1051 ;
1052 [label="X[620] <= -0.01\nentropy = 0.72\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0\n0.8, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
1050 -> 1052 ;
1053 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
1052 -> 1053 ;
1054 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
1052 -> 1054 ;
1055 [label="X[640] <= 0.01\nentropy = 0.73\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.86\n0.07, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
1045 -> 1055 ;
1056 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
1055 -> 1056 ;
1057 [label="X[856] <= -0.02\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.5, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
1055 -> 1057 ;
1058 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
1057 -> 1058 ;
1059 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
1057 -> 1059 ;
1060 [label="X[147] <= 0.0\nentropy = 3.41\nsamples = 2.3%\nvalue = [0.01, 0.03, 0.09, 0.01, 0.09, 0.04, 0.0, 0.12, 0.07\n0.07, 0.05, 0.15, 0.0, 0.08, 0.18]\nclass = Fox News"] ;
1012 -> 1060 ;
1061 [label="X[634] <= 0.0\nentropy = 2.88\nsamples = 1.5%\nvalue = [0.02, 0.0, 0.0, 0.02, 0.0, 0.04, 0.0, 0.14, 0.04\n0.06, 0.08, 0.22, 0.0, 0.12, 0.26]\nclass = Fox News"] ;
1060 -> 1061 ;
1062 [label="X[959] <= -0.01\nentropy = 1.77\nsamples = 0.7%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0\n0.04, 0.08, 0.0, 0.25, 0.54]\nclass = Fox News"] ;
1061 -> 1062 ;
1063 [label="X[922] <= 0.0\nentropy = 1.22\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.22, 0.0, 0.0, 0.0, 0.0\n0.11, 0.0, 0.0, 0.67, 0.0]\nclass = National Review"] ;
1062 -> 1063 ;
1064 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
1063 -> 1064 ;
1065 [label="X[150] <= 0.01\nentropy = 0.92\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.67, 0.0, 0.0, 0.0, 0.0\n0.33, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
1063 -> 1065 ;
1066 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
1065 -> 1066 ;
1067 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
1065 -> 1067 ;
1068 [label="X[987] <= -0.01\nentropy = 0.57\nsamples = 0.5%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.13, 0.0, 0.0, 0.87]\nclass = Fox News"] ;
1062 -> 1068 ;
1069 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
1068 -> 1069 ;
1070 [label="entropy = 0.0\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
1068 -> 1070 ;
1071 [label="X[285] <= 0.0\nentropy = 2.4\nsamples = 0.8%\nvalue = [0.04, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.27, 0.08\n0.12, 0.12, 0.35, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
1061 -> 1071 ;
1072 [label="X[670] <= 0.0\nentropy = 1.24\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.09, 0.0, 0.0, 0.0, 0.64, 0.0, 0.0\n0.27, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
1071 -> 1072 ;
1073 [label="X[544] <= 0.0\nentropy = 0.81\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.75, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
1072 -> 1073 ;
1074 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
1073 -> 1074 ;
1075 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
1073 -> 1075 ;
1076 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
1072 -> 1076 ;
1077 [label="X[333] <= -0.01\nentropy = 1.55\nsamples = 0.4%\nvalue = [0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13, 0.2\n0.0, 0.6, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
1071 -> 1077 ;
1078 [label="X[59] <= 0.03\nentropy = 1.46\nsamples = 0.2%\nvalue = [0.17, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33, 0.5\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
1077 -> 1078 ;
1079 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
1078 -> 1079 ;
1080 [label="X[334] <= -0.06\nentropy = 0.92\nsamples = 0.1%\nvalue = [0.33, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.67, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
1078 -> 1080 ;
1081 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
1080 -> 1081 ;
1082 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
1080 -> 1082 ;
1083 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
1077 -> 1083 ;
1084 [label="X[450] <= 0.0\nentropy = 2.5\nsamples = 0.8%\nvalue = [0.0, 0.08, 0.29, 0.0, 0.29, 0.04, 0.0, 0.08, 0.12\n0.08, 0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
1060 -> 1084 ;
1085 [label="X[521] <= -0.0\nentropy = 1.31\nsamples = 0.4%\nvalue = [0.0, 0.18, 0.0, 0.0, 0.64, 0.0, 0.0, 0.18, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
1084 -> 1085 ;
1086 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
1085 -> 1086 ;
1087 [label="X[564] <= -0.01\nentropy = 1.0\nsamples = 0.2%\nvalue = [0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
1085 -> 1087 ;
1088 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
1087 -> 1088 ;
1089 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
1087 -> 1089 ;
1090 [label="X[575] <= 0.0\nentropy = 1.67\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.54, 0.0, 0.0, 0.08, 0.0, 0.0, 0.23, 0.15\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
1084 -> 1090 ;
1091 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
1090 -> 1091 ;
1092 [label="X[894] <= -0.04\nentropy = 1.46\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.17, 0.0, 0.0, 0.5, 0.33\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
1090 -> 1092 ;
1093 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
1092 -> 1093 ;
1094 [label="X[297] <= 0.01\nentropy = 0.92\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.33, 0.0, 0.0, 0.0, 0.67\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
1092 -> 1094 ;
1095 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
1094 -> 1095 ;
1096 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
1094 -> 1096 ;
1097 [label="X[91] <= 0.02\nentropy = 3.49\nsamples = 12.5%\nvalue = [0.02, 0.13, 0.12, 0.04, 0.02, 0.07, 0.0, 0.08, 0.01\n0.16, 0.06, 0.06, 0.15, 0.07, 0.02]\nclass = Vox"] ;
759 -> 1097 ;
1098 [label="X[44] <= -0.03\nentropy = 3.34\nsamples = 8.5%\nvalue = [0.03, 0.12, 0.13, 0.03, 0.04, 0.04, 0.0, 0.02, 0.0\n0.19, 0.07, 0.04, 0.17, 0.11, 0.02]\nclass = Vox"] ;
1097 -> 1098 ;
1099 [label="X[636] <= 0.01\nentropy = 2.71\nsamples = 2.4%\nvalue = [0.08, 0.08, 0.01, 0.0, 0.05, 0.05, 0.0, 0.0, 0.0\n0.31, 0.0, 0.08, 0.07, 0.26, 0.0]\nclass = Vox"] ;
1098 -> 1099 ;
1100 [label="X[91] <= -0.02\nentropy = 2.54\nsamples = 1.5%\nvalue = [0.14, 0.05, 0.02, 0.0, 0.09, 0.02, 0.0, 0.0, 0.0\n0.07, 0.0, 0.14, 0.05, 0.43, 0.0]\nclass = National Review"] ;
1099 -> 1100 ;
1101 [label="X[77] <= -0.01\nentropy = 1.9\nsamples = 0.6%\nvalue = [0.29, 0.0, 0.0, 0.0, 0.24, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.35, 0.12, 0.0, 0.0]\nclass = Guardian"] ;
1100 -> 1101 ;
1102 [label="X[886] <= 0.0\nentropy = 0.99\nsamples = 0.3%\nvalue = [0.56, 0.0, 0.0, 0.0, 0.44, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
1101 -> 1102 ;
1103 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
1102 -> 1103 ;
1104 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
1102 -> 1104 ;
1105 [label="X[800] <= -0.02\nentropy = 0.81\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.75, 0.25, 0.0, 0.0]\nclass = Guardian"] ;
1101 -> 1105 ;
1106 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
1105 -> 1106 ;
1107 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
1105 -> 1107 ;
1108 [label="X[279] <= -0.0\nentropy = 1.52\nsamples = 0.9%\nvalue = [0.04, 0.07, 0.04, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0\n0.11, 0.0, 0.0, 0.0, 0.7, 0.0]\nclass = National Review"] ;
1100 -> 1108 ;
1109 [label="X[283] <= -0.01\nentropy = 1.79\nsamples = 0.3%\nvalue = [0.17, 0.0, 0.17, 0.0, 0.0, 0.17, 0.0, 0.0, 0.0, 0.5\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
1108 -> 1109 ;
1110 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
1109 -> 1110 ;
1111 [label="X[313] <= 0.02\nentropy = 1.58\nsamples = 0.2%\nvalue = [0.33, 0.0, 0.33, 0.0, 0.0, 0.33, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
1109 -> 1111 ;
1112 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
1111 -> 1112 ;
1113 [label="X[689] <= -0.01\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.5, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
1111 -> 1113 ;
1114 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
1113 -> 1114 ;
1115 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
1113 -> 1115 ;
1116 [label="X[634] <= -0.04\nentropy = 0.45\nsamples = 0.6%\nvalue = [0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.9, 0.0]\nclass = National Review"] ;
1108 -> 1116 ;
1117 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
1116 -> 1117 ;
1118 [label="entropy = 0.0\nsamples = 0.6%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
1116 -> 1118 ;
1119 [label="X[255] <= 0.01\nentropy = 1.44\nsamples = 0.9%\nvalue = [0.0, 0.13, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.67\n0.0, 0.0, 0.1, 0.0, 0.0]\nclass = Vox"] ;
1099 -> 1119 ;
1120 [label="entropy = 0.0\nsamples = 0.6%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
1119 -> 1120 ;
1121 [label="X[694] <= -0.01\nentropy = 1.57\nsamples = 0.3%\nvalue = [0.0, 0.4, 0.0, 0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.3, 0.0, 0.0]\nclass = Washington Post"] ;
1119 -> 1121 ;
1122 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
1121 -> 1122 ;
1123 [label="X[134] <= 0.02\nentropy = 1.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.5, 0.0, 0.0]\nclass = NPR"] ;
1121 -> 1123 ;
1124 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
1123 -> 1124 ;
1125 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
1123 -> 1125 ;
1126 [label="X[3] <= -0.01\nentropy = 3.24\nsamples = 6.1%\nvalue = [0.01, 0.14, 0.17, 0.04, 0.03, 0.03, 0.01, 0.03, 0.0\n0.14, 0.1, 0.02, 0.21, 0.04, 0.03]\nclass = Atlantic"] ;
1098 -> 1126 ;
1127 [label="X[864] <= -0.01\nentropy = 2.75\nsamples = 3.1%\nvalue = [0.02, 0.19, 0.05, 0.01, 0.02, 0.01, 0.0, 0.01, 0.0\n0.16, 0.02, 0.05, 0.38, 0.07, 0.01]\nclass = Atlantic"] ;
1126 -> 1127 ;
1128 [label="X[584] <= -0.0\nentropy = 2.61\nsamples = 0.9%\nvalue = [0.04, 0.3, 0.17, 0.04, 0.0, 0.04, 0.0, 0.04, 0.0\n0.0, 0.0, 0.13, 0.0, 0.22, 0.0]\nclass = Washington Post"] ;
1127 -> 1128 ;
1129 [label="X[306] <= -0.01\nentropy = 1.24\nsamples = 0.4%\nvalue = [0.0, 0.64, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.27, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
1128 -> 1129 ;
1130 [label="X[980] <= -0.01\nentropy = 0.81\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.75, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
1129 -> 1130 ;
1131 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
1130 -> 1131 ;
1132 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
1130 -> 1132 ;
1133 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
1129 -> 1133 ;
1134 [label="X[66] <= -0.01\nentropy = 1.95\nsamples = 0.5%\nvalue = [0.08, 0.0, 0.33, 0.0, 0.0, 0.08, 0.0, 0.08, 0.0, 0.0\n0.0, 0.0, 0.0, 0.42, 0.0]\nclass = National Review"] ;
1128 -> 1134 ;
1135 [label="X[512] <= -0.01\nentropy = 1.25\nsamples = 0.3%\nvalue = [0.17, 0.0, 0.67, 0.0, 0.0, 0.0, 0.0, 0.17, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
1134 -> 1135 ;
1136 [label="X[86] <= -0.0\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
1135 -> 1136 ;
1137 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
1136 -> 1137 ;
1138 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
1136 -> 1138 ;
1139 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
1135 -> 1139 ;
1140 [label="X[842] <= -0.01\nentropy = 0.65\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.17, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.83, 0.0]\nclass = National Review"] ;
1134 -> 1140 ;
1141 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
1140 -> 1141 ;
1142 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
1140 -> 1142 ;
1143 [label="X[301] <= 0.01\nentropy = 2.07\nsamples = 2.2%\nvalue = [0.02, 0.15, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.22\n0.03, 0.02, 0.51, 0.02, 0.02]\nclass = Atlantic"] ;
1127 -> 1143 ;
1144 [label="X[959] <= 0.01\nentropy = 1.48\nsamples = 1.7%\nvalue = [0.02, 0.17, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02\n0.04, 0.0, 0.7, 0.02, 0.02]\nclass = Atlantic"] ;
1143 -> 1144 ;
1145 [label="X[434] <= 0.03\nentropy = 0.56\nsamples = 1.2%\nvalue = [0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03\n0.0, 0.0, 0.91, 0.0, 0.03]\nclass = Atlantic"] ;
1144 -> 1145 ;
1146 [label="entropy = 0.0\nsamples = 1.0%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
1145 -> 1146 ;
1147 [label="X[259] <= -0.03\nentropy = 1.58\nsamples = 0.2%\nvalue = [0.0, 0.33, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33\n0.0, 0.0, 0.0, 0.0, 0.33]\nclass = Washington Post"] ;
1145 -> 1147 ;
1148 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
1147 -> 1148 ;
1149 [label="X[286] <= -0.01\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.5]\nclass = Washington Post"] ;
1147 -> 1149 ;
1150 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
1149 -> 1150 ;
1151 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
1149 -> 1151 ;
1152 [label="X[228] <= 0.02\nentropy = 1.49\nsamples = 0.5%\nvalue = [0.09, 0.64, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.18, 0.0, 0.0, 0.09, 0.0]\nclass = Washington Post"] ;
1144 -> 1152 ;
1153 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
1152 -> 1153 ;
1154 [label="X[907] <= 0.01\nentropy = 1.5\nsamples = 0.2%\nvalue = [0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.5, 0.0, 0.0, 0.25, 0.0]\nclass = Talking Points Memo"] ;
1152 -> 1154 ;
1155 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
1154 -> 1155 ;
1156 [label="X[865] <= -0.0\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.5, 0.0]\nclass = Breitbart"] ;
1154 -> 1156 ;
1157 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
1156 -> 1157 ;
1158 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
1156 -> 1158 ;
1159 [label="X[112] <= -0.02\nentropy = 1.51\nsamples = 0.6%\nvalue = [0.0, 0.11, 0.0, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.68\n0.0, 0.05, 0.05, 0.0, 0.0]\nclass = Vox"] ;
1143 -> 1159 ;
1160 [label="X[31] <= 0.0\nentropy = 1.92\nsamples = 0.2%\nvalue = [0.0, 0.33, 0.0, 0.0, 0.33, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.17, 0.17, 0.0, 0.0]\nclass = Washington Post"] ;
1159 -> 1160 ;
1161 [label="X[794] <= 0.01\nentropy = 0.92\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.67, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.33, 0.0, 0.0]\nclass = Buzzfeed News"] ;
1160 -> 1161 ;
1162 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
1161 -> 1162 ;
1163 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
1161 -> 1163 ;
1164 [label="X[731] <= -0.02\nentropy = 0.92\nsamples = 0.1%\nvalue = [0.0, 0.67, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.33, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
1160 -> 1164 ;
1165 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
1164 -> 1165 ;
1166 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
1164 -> 1166 ;
1167 [label="entropy = 0.0\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
1159 -> 1167 ;
1168 [label="X[308] <= 0.0\nentropy = 3.07\nsamples = 3.0%\nvalue = [0.0, 0.09, 0.29, 0.06, 0.03, 0.05, 0.01, 0.04, 0.0\n0.12, 0.18, 0.0, 0.05, 0.02, 0.04]\nclass = CNN"] ;
1126 -> 1168 ;
1169 [label="X[164] <= 0.01\nentropy = 2.61\nsamples = 1.8%\nvalue = [0.0, 0.0, 0.46, 0.08, 0.03, 0.08, 0.0, 0.07, 0.0\n0.03, 0.13, 0.0, 0.05, 0.03, 0.03]\nclass = CNN"] ;
1168 -> 1169 ;
1170 [label="X[208] <= -0.02\nentropy = 1.78\nsamples = 1.3%\nvalue = [0.0, 0.0, 0.65, 0.07, 0.0, 0.12, 0.0, 0.0, 0.0, 0.0\n0.02, 0.0, 0.05, 0.05, 0.05]\nclass = CNN"] ;
1169 -> 1170 ;
1171 [label="X[504] <= -0.01\nentropy = 2.45\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.3, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0\n0.1, 0.0, 0.2, 0.1, 0.2]\nclass = Business Insider"] ;
1170 -> 1171 ;
1172 [label="X[608] <= 0.0\nentropy = 0.97\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.4, 0.0, 0.0]\nclass = Business Insider"] ;
1171 -> 1172 ;
1173 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
1172 -> 1173 ;
1174 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
1172 -> 1174 ;
1175 [label="X[968] <= -0.0\nentropy = 1.92\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0\n0.2, 0.0, 0.0, 0.2, 0.4]\nclass = Fox News"] ;
1171 -> 1175 ;
1176 [label="X[881] <= 0.01\nentropy = 0.92\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.33, 0.0, 0.0, 0.0, 0.67]\nclass = Fox News"] ;
1175 -> 1176 ;
1177 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
1176 -> 1177 ;
1178 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
1176 -> 1178 ;
1179 [label="X[502] <= -0.01\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.5, 0.0]\nclass = NPR"] ;
1175 -> 1179 ;
1180 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
1179 -> 1180 ;
1181 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
1179 -> 1181 ;
1182 [label="X[950] <= -0.02\nentropy = 0.72\nsamples = 0.8%\nvalue = [0.0, 0.0, 0.85, 0.0, 0.0, 0.12, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.03, 0.0]\nclass = CNN"] ;
1170 -> 1182 ;
1183 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
1182 -> 1183 ;
1184 [label="X[397] <= 0.06\nentropy = 0.22\nsamples = 0.7%\nvalue = [0.0, 0.0, 0.97, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.03, 0.0]\nclass = CNN"] ;
1182 -> 1184 ;
1185 [label="entropy = 0.0\nsamples = 0.6%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
1184 -> 1185 ;
1186 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
1184 -> 1186 ;
1187 [label="X[323] <= -0.02\nentropy = 2.3\nsamples = 0.5%\nvalue = [0.0, 0.0, 0.0, 0.11, 0.11, 0.0, 0.0, 0.22, 0.0, 0.11\n0.39, 0.0, 0.06, 0.0, 0.0]\nclass = Talking Points Memo"] ;
1169 -> 1187 ;
1188 [label="X[8] <= -0.01\nentropy = 0.76\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.22, 0.0, 0.0, 0.0, 0.0, 0.0\n0.78, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
1187 -> 1188 ;
1189 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
1188 -> 1189 ;
1190 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
1188 -> 1190 ;
1191 [label="X[89] <= -0.02\nentropy = 1.84\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.22, 0.0, 0.0, 0.0, 0.44, 0.0, 0.22\n0.0, 0.0, 0.11, 0.0, 0.0]\nclass = Reuters"] ;
1187 -> 1191 ;
1192 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
1191 -> 1192 ;
1193 [label="X[590] <= -0.01\nentropy = 1.52\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4\n0.0, 0.0, 0.2, 0.0, 0.0]\nclass = Business Insider"] ;
1191 -> 1193 ;
1194 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
1193 -> 1194 ;
1195 [label="X[983] <= -0.02\nentropy = 0.92\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.67, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.33, 0.0, 0.0]\nclass = Business Insider"] ;
1193 -> 1195 ;
1196 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
1195 -> 1196 ;
1197 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
1195 -> 1197 ;
1198 [label="X[276] <= -0.02\nentropy = 2.45\nsamples = 1.2%\nvalue = [0.0, 0.26, 0.0, 0.03, 0.03, 0.0, 0.03, 0.0, 0.0\n0.26, 0.26, 0.0, 0.06, 0.0, 0.06]\nclass = Washington Post"] ;
1168 -> 1198 ;
1199 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
1198 -> 1199 ;
1200 [label="X[250] <= 0.0\nentropy = 2.2\nsamples = 0.9%\nvalue = [0.0, 0.36, 0.0, 0.04, 0.04, 0.0, 0.04, 0.0, 0.0\n0.36, 0.0, 0.0, 0.08, 0.0, 0.08]\nclass = Washington Post"] ;
1198 -> 1200 ;
1201 [label="X[207] <= -0.01\nentropy = 1.04\nsamples = 0.4%\nvalue = [0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08\n0.0, 0.0, 0.0, 0.0, 0.17]\nclass = Washington Post"] ;
1200 -> 1201 ;
1202 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
1201 -> 1202 ;
1203 [label="X[720] <= 0.0\nentropy = 0.92\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33\n0.0, 0.0, 0.0, 0.0, 0.67]\nclass = Fox News"] ;
1201 -> 1203 ;
1204 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
1203 -> 1204 ;
1205 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
1203 -> 1205 ;
1206 [label="X[25] <= -0.02\nentropy = 1.7\nsamples = 0.5%\nvalue = [0.0, 0.0, 0.0, 0.08, 0.08, 0.0, 0.08, 0.0, 0.0, 0.62\n0.0, 0.0, 0.15, 0.0, 0.0]\nclass = Vox"] ;
1200 -> 1206 ;
1207 [label="X[637] <= -0.01\nentropy = 1.92\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.2, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0\n0.0, 0.0, 0.4, 0.0, 0.0]\nclass = Atlantic"] ;
1206 -> 1207 ;
1208 [label="X[957] <= -0.0\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
1207 -> 1208 ;
1209 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Buzzfeed News"] ;
1208 -> 1209 ;
1210 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
1208 -> 1210 ;
1211 [label="X[707] <= -0.0\nentropy = 0.92\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33, 0.0, 0.0, 0.0\n0.0, 0.0, 0.67, 0.0, 0.0]\nclass = Atlantic"] ;
1207 -> 1211 ;
1212 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
1211 -> 1212 ;
1213 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
1211 -> 1213 ;
1214 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
1206 -> 1214 ;
1215 [label="X[42] <= -0.01\nentropy = 3.22\nsamples = 4.1%\nvalue = [0.0, 0.15, 0.09, 0.07, 0.0, 0.13, 0.0, 0.19, 0.03\n0.1, 0.02, 0.1, 0.1, 0.01, 0.01]\nclass = Reuters"] ;
1097 -> 1215 ;
1216 [label="X[160] <= 0.02\nentropy = 2.28\nsamples = 1.8%\nvalue = [0.0, 0.18, 0.04, 0.02, 0.0, 0.14, 0.0, 0.42, 0.0\n0.02, 0.0, 0.18, 0.0, 0.02, 0.0]\nclass = Reuters"] ;
1215 -> 1216 ;
1217 [label="X[431] <= 0.01\nentropy = 2.15\nsamples = 0.9%\nvalue = [0.0, 0.22, 0.07, 0.04, 0.0, 0.26, 0.0, 0.0, 0.0\n0.04, 0.0, 0.37, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
1216 -> 1217 ;
1218 [label="X[599] <= 0.01\nentropy = 1.22\nsamples = 0.6%\nvalue = [0.0, 0.35, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.59, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
1217 -> 1218 ;
1219 [label="entropy = 0.0\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
1218 -> 1219 ;
1220 [label="X[432] <= 0.02\nentropy = 0.59\nsamples = 0.2%\nvalue = [0.0, 0.86, 0.0, 0.14, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
1218 -> 1220 ;
1221 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
1220 -> 1221 ;
1222 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
1220 -> 1222 ;
1223 [label="X[616] <= 0.01\nentropy = 1.16\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.2, 0.0, 0.0, 0.7, 0.0, 0.0, 0.0, 0.1\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
1217 -> 1223 ;
1224 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
1223 -> 1224 ;
1225 [label="X[756] <= -0.0\nentropy = 0.92\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.67, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
1223 -> 1225 ;
1226 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
1225 -> 1226 ;
1227 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
1225 -> 1227 ;
1228 [label="X[39] <= -0.04\nentropy = 0.97\nsamples = 0.9%\nvalue = [0.0, 0.13, 0.0, 0.0, 0.0, 0.03, 0.0, 0.8, 0.0, 0.0\n0.0, 0.0, 0.0, 0.03, 0.0]\nclass = Reuters"] ;
1216 -> 1228 ;
1229 [label="X[302] <= -0.01\nentropy = 1.25\nsamples = 0.2%\nvalue = [0.0, 0.67, 0.0, 0.0, 0.0, 0.17, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.17, 0.0]\nclass = Washington Post"] ;
1228 -> 1229 ;
1230 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
1229 -> 1230 ;
1231 [label="X[899] <= -0.0\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.5, 0.0]\nclass = NPR"] ;
1229 -> 1231 ;
1232 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
1231 -> 1232 ;
1233 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
1231 -> 1233 ;
1234 [label="entropy = 0.0\nsamples = 0.7%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Reuters"] ;
1228 -> 1234 ;
1235 [label="X[240] <= 0.01\nentropy = 3.06\nsamples = 2.3%\nvalue = [0.0, 0.12, 0.13, 0.12, 0.0, 0.12, 0.0, 0.0, 0.06\n0.16, 0.04, 0.03, 0.19, 0.0, 0.01]\nclass = Atlantic"] ;
1215 -> 1235 ;
1236 [label="X[937] <= 0.01\nentropy = 2.53\nsamples = 1.4%\nvalue = [0.0, 0.03, 0.24, 0.19, 0.0, 0.0, 0.0, 0.0, 0.11\n0.03, 0.03, 0.03, 0.32, 0.0, 0.03]\nclass = Atlantic"] ;
1235 -> 1236 ;
1237 [label="X[128] <= -0.01\nentropy = 2.24\nsamples = 1.0%\nvalue = [0.0, 0.0, 0.38, 0.29, 0.0, 0.0, 0.0, 0.0, 0.17, 0.04\n0.04, 0.0, 0.04, 0.0, 0.04]\nclass = CNN"] ;
1236 -> 1237 ;
1238 [label="X[244] <= -0.02\nentropy = 1.36\nsamples = 0.4%\nvalue = [0.0, 0.0, 0.0, 0.7, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1\n0.1, 0.0, 0.1, 0.0, 0.0]\nclass = Business Insider"] ;
1237 -> 1238 ;
1239 [label="X[65] <= -0.04\nentropy = 1.58\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33\n0.33, 0.0, 0.33, 0.0, 0.0]\nclass = Vox"] ;
1238 -> 1239 ;
1240 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
1239 -> 1240 ;
1241 [label="X[520] <= -0.01\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5\n0.0, 0.0, 0.5, 0.0, 0.0]\nclass = Vox"] ;
1239 -> 1241 ;
1242 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
1241 -> 1242 ;
1243 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
1241 -> 1243 ;
1244 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
1238 -> 1244 ;
1245 [label="X[485] <= -0.01\nentropy = 1.2\nsamples = 0.6%\nvalue = [0.0, 0.0, 0.64, 0.0, 0.0, 0.0, 0.0, 0.0, 0.29, 0.0\n0.0, 0.0, 0.0, 0.0, 0.07]\nclass = CNN"] ;
1237 -> 1245 ;
1246 [label="X[203] <= -0.03\nentropy = 0.72\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0\n0.0, 0.0, 0.0, 0.0, 0.2]\nclass = New York Post"] ;
1245 -> 1246 ;
1247 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 1.0]\nclass = Fox News"] ;
1246 -> 1247 ;
1248 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
1246 -> 1248 ;
1249 [label="entropy = 0.0\nsamples = 0.4%\nvalue = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = CNN"] ;
1245 -> 1249 ;
1250 [label="X[536] <= -0.01\nentropy = 0.77\nsamples = 0.4%\nvalue = [0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.08, 0.85, 0.0, 0.0]\nclass = Atlantic"] ;
1236 -> 1250 ;
1251 [label="X[531] <= 0.02\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.5, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
1250 -> 1251 ;
1252 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
1251 -> 1252 ;
1253 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
1251 -> 1253 ;
1254 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
1250 -> 1254 ;
1255 [label="X[681] <= 0.01\nentropy = 2.28\nsamples = 0.9%\nvalue = [0.0, 0.23, 0.0, 0.03, 0.0, 0.27, 0.0, 0.0, 0.0, 0.33\n0.07, 0.03, 0.03, 0.0, 0.0]\nclass = Vox"] ;
1235 -> 1255 ;
1256 [label="X[528] <= -0.0\nentropy = 1.47\nsamples = 0.6%\nvalue = [0.0, 0.37, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.53\n0.0, 0.05, 0.05, 0.0, 0.0]\nclass = Vox"] ;
1255 -> 1256 ;
1257 [label="X[315] <= -0.01\nentropy = 0.99\nsamples = 0.3%\nvalue = [0.0, 0.78, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.11, 0.11, 0.0, 0.0]\nclass = Washington Post"] ;
1256 -> 1257 ;
1258 [label="X[559] <= -0.03\nentropy = 1.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.5, 0.5, 0.0, 0.0]\nclass = Guardian"] ;
1257 -> 1258 ;
1259 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 1.0, 0.0, 0.0]\nclass = Atlantic"] ;
1258 -> 1259 ;
1260 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 1.0, 0.0, 0.0, 0.0]\nclass = Guardian"] ;
1258 -> 1260 ;
1261 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
1257 -> 1261 ;
1262 [label="entropy = 0.0\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Vox"] ;
1256 -> 1262 ;
1263 [label="X[393] <= -0.02\nentropy = 1.1\nsamples = 0.3%\nvalue = [0.0, 0.0, 0.0, 0.09, 0.0, 0.73, 0.0, 0.0, 0.0, 0.0\n0.18, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
1255 -> 1263 ;
1264 [label="X[826] <= 0.01\nentropy = 0.92\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.33, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.67, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
1263 -> 1264 ;
1265 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Business Insider"] ;
1264 -> 1265 ;
1266 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n1.0, 0.0, 0.0, 0.0, 0.0]\nclass = Talking Points Memo"] ;
1264 -> 1266 ;
1267 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = NPR"] ;
1263 -> 1267 ;
1268 [label="X[524] <= 0.03\nentropy = 0.66\nsamples = 3.8%\nvalue = [0.02, 0.06, 0.0, 0.0, 0.0, 0.0, 0.89, 0.0, 0.02, 0.0\n0.0, 0.0, 0.0, 0.01, 0.0]\nclass = New York Times"] ;
0 -> 1268 [labeldistance=2.5, labelangle=-45, headlabel="False"] ;
1269 [label="X[215] <= 0.04\nentropy = 0.36\nsamples = 3.5%\nvalue = [0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.95, 0.0, 0.02, 0.0\n0.0, 0.0, 0.0, 0.01, 0.0]\nclass = New York Times"] ;
1268 -> 1269 ;
1270 [label="entropy = 0.0\nsamples = 3.3%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
1269 -> 1270 ;
1271 [label="X[167] <= -0.0\nentropy = 1.92\nsamples = 0.2%\nvalue = [0.33, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17, 0.0, 0.33, 0.0\n0.0, 0.0, 0.0, 0.17, 0.0]\nclass = Breitbart"] ;
1269 -> 1271 ;
1272 [label="X[983] <= -0.01\nentropy = 0.92\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33, 0.0, 0.67, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
1271 -> 1272 ;
1273 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Post"] ;
1272 -> 1273 ;
1274 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
1272 -> 1274 ;
1275 [label="X[583] <= -0.01\nentropy = 0.92\nsamples = 0.1%\nvalue = [0.67, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.33, 0.0]\nclass = Breitbart"] ;
1271 -> 1275 ;
1276 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 1.0, 0.0]\nclass = National Review"] ;
1275 -> 1276 ;
1277 [label="entropy = 0.0\nsamples = 0.1%\nvalue = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Breitbart"] ;
1275 -> 1277 ;
1278 [label="X[743] <= 0.01\nentropy = 0.95\nsamples = 0.3%\nvalue = [0.0, 0.64, 0.0, 0.0, 0.0, 0.0, 0.36, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
1268 -> 1278 ;
1279 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = New York Times"] ;
1278 -> 1279 ;
1280 [label="entropy = 0.0\nsamples = 0.2%\nvalue = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n0.0, 0.0, 0.0, 0.0, 0.0]\nclass = Washington Post"] ;
1278 -> 1280 ;
}